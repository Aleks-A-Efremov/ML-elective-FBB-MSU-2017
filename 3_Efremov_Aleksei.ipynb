{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "Результат работы − отчет в формате ноутбуков IPython (ipynb-файл). Код пишется на Python3. Постарайтесь сделать ваш отчет интересным рассказом, последовательно отвечающим на вопросы из заданий. Помимо ответов на вопросы, в отчете также должен быть код, однако чем меньше кода, тем лучше всем: мне − меньше проверять, вам — проще найти ошибку или дополнить эксперимент. При проверке оценивается четкость ответов на вопросы, аккуратность отчета и кода.    \n",
    "Выполнение лабораторных работ занимает значительное время, поэтому не рекомендуем оставлять их на последний вечер перед сдачей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценивание и штрафы\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) получают за всю лабораторную работу 0 баллов. Если вы нашли решение какого-то из заданий в открытом источнике, необходимо в комментариях к коду указать ссылку на этот источник (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "**Важно!!!** Прочитайте [руководство по написанию кода](https://pythonworld.ru/osnovy/pep-8-rukovodstvo-po-napisaniyu-koda-na-python.html). Работы, где будут грубо нарушены принципы оформления кода, будут штрафоваться!    \n",
    "Также помните, что самая главная ошибка, которую надо избегать, - дублирование кода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Правила сдачи\n",
    "Выполненную работу следует отправить на почту `nikmort@ya.ru` с указанием темы `[FBB hw <номер домашнего задани> Surname Name]`, например `FBB hw 2 Ivanov Petr`. Название отправляемого файла должно иметь следующий формат: `N_Surname_Name.ipynb`, где `N` — номер домашнего задания. Например, `2_Ivanov_Petr.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "В этом задании вам будет предложено ознакомиться с двумя мощными библиотеками для машинного обучения - xgboost и keras. В первой реализован градиентный бустинг, во второй - нейронные сети. Ваша задача будет попробовать применить каждую из моделей к задаче мультиклассовой классификации и исследовать поведение моделей в зависимости от выбора параметров. Работать вам предстоит с  датасетом, составленным из рукописных \"картинок\" цифр. Он вам знаком по предыдущему заданию (можете брать код для работы с ним оттуда). Только в этот раз мы возьмем его полную версию - картинки будут размером 28х28 вместо 8х8, а общее число картинок - 42000 вместо 1797."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите датасет и разбейте его на выборки для обучения и контроля.\n",
    "\n",
    "Для ускорения работы возьмите небольшую часть датасета, например, 3%. Отладьте на ней код, а потом запустите расчеты на больших данных.\n",
    "\n",
    "Скорее всего, вычисления будут трудоемкими, если брать весь датасет, поэтому для итоговых вычислений можете взять только его часть (но не меньше 30%).\n",
    "\n",
    "Обратите внимание, что наблюдаемые результаты могут сильно зависеть от того, делаете ли вы эксперимент на маленьких или больших данных. Так, на выборке размера 100 ваш классификатор может легко переобучиться, в то время как на выборке размера 10000 этот эффект может не наблюдаться. Поэтому делайте выводы после запуска расчетов на больших данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sample_frac):\n",
    "    data = pd.read_csv('train.csv')\n",
    "    data = data.sample(frac=sample_frac)\n",
    "    labels = data['label'].values\n",
    "    digits = data.drop('label', 1).values\n",
    "    return digits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits, labels = load_data(0.3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits, labels, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте в качестве базового решения посмотрим на известные нам алгоритмы. Возьмите kNN и Random Forest. Обучите их (гиперпараметры оставьте по умолчанию), подсчитайте точность и logloss на тестовой выборке. Какой алгоритм дал лучший результат? Как различаются алгоритмы по качеству и времени обучения и предсказания?\n",
    "\n",
    "hint: используйте %time для измерения времени работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 3.58 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 3.34 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "y_knn = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941904761905\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_pred=y_knn, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 12.6 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "y_knn_proba = knn.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.700921574691\n"
     ]
    }
   ],
   "source": [
    "print(log_loss(y_pred=y_knn_proba, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 3.34 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "rf = RandomForestClassifier(n_jobs=4, random_state=21)\n",
    "rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 12.6 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "y_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904126984127\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_pred=y_rf, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 3.34 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "y_rf_proba = rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.709844545268\n"
     ]
    }
   ],
   "source": [
    "print(log_loss(y_pred=y_rf_proba, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "del y_knn, y_knn_proba, y_rf, y_rf_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее целевой метрикой для нас будет logloss. Точность также будем вычислять, как более интерпретируемую метрику."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "(6 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Установите библиотеку xgboost.](https://xgboost.readthedocs.io/en/latest/build.html) Реализация бустинга есть и в sklearn, но в ней уделено сильно меньше внимания регуляризации и скорости, поэтому мы будем использовать xgboost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмите классификатор с настройками по умолчанию (рекомендуется установить n_jobs на -1 для ускорения расчета). Документацию вы можете найти [тут](http://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = XGBClassifier(n_jobs=4, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите его и предскажите метки для тестовой выборки. Выведите logloss, точность классификации и confusion matrix для обученного классификатора. Сравните время обучения и предсказания с предыдущими классификаторами. Склонен ли классификатор давать больше ошибок на определенных классах?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost.fit(X_train, y_train)\n",
    "y_xgb = xgboost.predict(X_test)\n",
    "y_xgb_proba = xgboost.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92380952381 \n",
      "\n",
      "Logloss: 0.273825460057 \n",
      "\n",
      "Confusion Matrix:\n",
      " [[298   0   3   2   1   4   3   2   1   1]\n",
      " [  0 344   2   2   0   0   2   1   4   0]\n",
      " [  0   1 293   6   3   2   2   6   2   1]\n",
      " [  1   1   4 269   1  11   0   1   8   3]\n",
      " [  2   0   1   0 294   1   1   1   2   9]\n",
      " [  0   1   1   8   0 267   5   0   7   4]\n",
      " [  3   1   4   2   4   3 289   0   3   0]\n",
      " [  0   0   8  10   0   2   1 310   1   6]\n",
      " [  3   2   4   6   2   5   1   0 279   2]\n",
      " [  0   0   1   6  16   3   0  13   4 267]]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(y_xgb, y_test),'\\n')\n",
    "print('Logloss:', log_loss(y_pred=y_xgb_proba, y_true=y_test), '\\n')\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_xgb, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Визуализируйте](https://stackoverflow.com/questions/33282368/plotting-a-2d-heatmap-with-matplotlib) важность признаков (feature importances) на картинке 28х28. Какие пиксели изображения наиболее важны для классификации? Как вы думаете, почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4fd942908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xgboost.feature_importances_.reshape(28,28), cmap='hot', interpolation='nearest')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Черные пиксели совсем не важны для классификации, т.к. они пустые на большинстве или на всех картинках. \n",
    "Центральные пиксели важны, т.к. там и располагаются цифры. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем потюнить XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Выберите относительно большую learning_rate ($\\eta\\in[0.05,0.3]$), подберите оптимальное число деревьев для выбранного $\\eta$. В методе `fit` задайте `eval_metric`, равное `mlogloss`, в `eval_set` передайте `[(X_test, y_test)]`; таким образом, вы сможете получать качество вашей классификации после каждого обученного базового классификатора. Вы можете регулировать \"болтливость\" метода обучения с помощью параметры `verbose` (например, задать его равным 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.86833\n",
      "[10]\tvalidation_0-mlogloss:0.753074\n",
      "[20]\tvalidation_0-mlogloss:0.483065\n",
      "[30]\tvalidation_0-mlogloss:0.368815\n",
      "[40]\tvalidation_0-mlogloss:0.307262\n",
      "[50]\tvalidation_0-mlogloss:0.268673\n",
      "[60]\tvalidation_0-mlogloss:0.244123\n",
      "[70]\tvalidation_0-mlogloss:0.225746\n",
      "[80]\tvalidation_0-mlogloss:0.212702\n",
      "[90]\tvalidation_0-mlogloss:0.201838\n",
      "[100]\tvalidation_0-mlogloss:0.193651\n",
      "[110]\tvalidation_0-mlogloss:0.186563\n",
      "[120]\tvalidation_0-mlogloss:0.180492\n",
      "[130]\tvalidation_0-mlogloss:0.176075\n",
      "[140]\tvalidation_0-mlogloss:0.172745\n",
      "[150]\tvalidation_0-mlogloss:0.169059\n",
      "[160]\tvalidation_0-mlogloss:0.166019\n",
      "[170]\tvalidation_0-mlogloss:0.163778\n",
      "[180]\tvalidation_0-mlogloss:0.162256\n",
      "[190]\tvalidation_0-mlogloss:0.161497\n",
      "[200]\tvalidation_0-mlogloss:0.161058\n",
      "[210]\tvalidation_0-mlogloss:0.160342\n",
      "[220]\tvalidation_0-mlogloss:0.159448\n",
      "[230]\tvalidation_0-mlogloss:0.158842\n",
      "[240]\tvalidation_0-mlogloss:0.158488\n",
      "[250]\tvalidation_0-mlogloss:0.157911\n",
      "[260]\tvalidation_0-mlogloss:0.157496\n",
      "[270]\tvalidation_0-mlogloss:0.157525\n",
      "[280]\tvalidation_0-mlogloss:0.157348\n",
      "[290]\tvalidation_0-mlogloss:0.157144\n",
      "[300]\tvalidation_0-mlogloss:0.157102\n",
      "[310]\tvalidation_0-mlogloss:0.157072\n",
      "[320]\tvalidation_0-mlogloss:0.157296\n",
      "[330]\tvalidation_0-mlogloss:0.156966\n",
      "[340]\tvalidation_0-mlogloss:0.156723\n",
      "[350]\tvalidation_0-mlogloss:0.156947\n",
      "[360]\tvalidation_0-mlogloss:0.156894\n",
      "[370]\tvalidation_0-mlogloss:0.157199\n",
      "[380]\tvalidation_0-mlogloss:0.157507\n",
      "[390]\tvalidation_0-mlogloss:0.157556\n",
      "[400]\tvalidation_0-mlogloss:0.157916\n",
      "[410]\tvalidation_0-mlogloss:0.158275\n",
      "[420]\tvalidation_0-mlogloss:0.158549\n",
      "[430]\tvalidation_0-mlogloss:0.158625\n",
      "[440]\tvalidation_0-mlogloss:0.158891\n",
      "[450]\tvalidation_0-mlogloss:0.15919\n",
      "[460]\tvalidation_0-mlogloss:0.159346\n",
      "[470]\tvalidation_0-mlogloss:0.159785\n",
      "[480]\tvalidation_0-mlogloss:0.159781\n",
      "[490]\tvalidation_0-mlogloss:0.159979\n",
      "[499]\tvalidation_0-mlogloss:0.160223\n"
     ]
    }
   ],
   "source": [
    "xgboost = XGBClassifier(learning_rate=0.2, n_jobs=4, random_state=12, n_estimators=500)\n",
    "xgboost.fit(X_train, y_train, eval_metric='mlogloss', eval_set=[(X_test, y_test)], verbose=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте график зависимости качества классификации от числа базовых классификаторов (для этого можете воспользоваться методом `evals_result`). Для большей наглядности можете отдельно отобразить график по последним 60 точкам. Как вам кажется, какое количество базовых классификаторов будет оптимальным? \n",
    "\n",
    "Скорее всего, вы будете наблюдать выход качества на плато. Если вы хотите посмотреть на переобучение, попробуйте позапускать обучение на маленькой подвыборке (100-300 элементов), возможно, вы сможете его отловить. На больших сложных выборках переобучение обычно возникает при существенном количестве базовых классификаторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd4XNW19/Hv0qgXy0WSbSx3uYMLuBGawQFMCaTAjSkB\ngoNDQm5yU0hIuQTSXki4Icm9hF4MhBqIMb2YGoptGWzjbrlLli3JRbK6NLPfP+bIHtuyLNnSzGj0\n+zzPPDNnnz3n7KUZaWnvfYo55xARETmcuEg3QEREopsShYiItEiJQkREWqREISIiLVKiEBGRFilR\niIhIi5QoRESkRUoUIiLSIiUKERFpUXykG9AesrKy3KBBgyLdDBGRTmXx4sVlzrnsI9WLiUQxaNAg\n8vPzI90MEZFOxcw2t6aehp5ERKRFShQiItKiLp0o3ly5g4m/e4uNZVWRboqISNTq0okCoKyyjqq6\nxkg3Q0QkanXpRJGa6ANQohARaYESBVDd4I9wS0REolcXTxTBo4Or65QoREQOp4snCq9HUa+hJxGR\nw1GiAKrr1aMQETmcLp4ovKEnJQoRkcPq0okiOSEOMw09iYi0pEsnCjMjNcGnHoWISAu6dKIASE2K\nV49CRKQFShSJ6lGIiLSkyyeKlAQfVTqPQkTksLp8okhLiqemQUNPIiKH0+UTRWqij0r1KEREDqvL\nJ4puKQnsrW2IdDNERKKWEkVyAhU1GnoSETmcsCYKM3vIzErMbPlh1k8zs3IzW+I9bu7oNnVLiadC\nPQoRkcOKD/P+HgH+D3i0hTofOOcuDE9zgj2K+sYAtQ1+khN84dqtiEinEdYehXPufWBXOPd5JN1S\nEgDUqxAROYxonKM42cyWmtmrZjbmcJXMbLaZ5ZtZfmlp6VHvrFtysFOleQoRkeZFW6L4FBjonBsH\n/C8w93AVnXP3OecmOucmZmdnH/UO1aMQEWlZVCUK51yFc67Se/0KkGBmWR25z27JXqKoUaIQEWlO\nVCUKM+tjZua9nkywfTs7cp+ZKd7QU62GnkREmhPWo57M7ElgGpBlZoXAr4EEAOfcPcAlwHfMrBGo\nAWY651xHtqlp6Km8ur4jdyMi0mmFNVE45y47wvr/I3j4bNj0SE0EYFeVhp5ERJoTVUNPkZDgi6Nb\ncjy7quoi3RQRkajU5RMFQK/0JHZWaehJRKQ5ShRAj9QEdilRiIg0S4kC6JmWpEQhInIYShRAr7RE\nJQoRkcNQogB6pgcTRQcfiSsi0ikpUQA5GUk0Bpx6FSIizVCiAPpmJgNQXF4b4ZaIiEQfJQqgT2YK\nANuVKEREDqFEAfTp5vUoKpQoREQOpkQBZGck4YszdqhHISJyCCUKwBdn5GQkaY5CRKQZShSePpnJ\nbK+oiXQzRESijhKFp0+3ZE1mi4g0Q4nC0yczmeLyWp10JyJyECUKT9/MZKrr/eyt053uRERCKVF4\ndC6FiEjzwpoozOwhMysxs+WHWW9m9jczKzCzZWZ2YrjaltsjmCi27KwO1y5FRDqFcPcoHgFmtLD+\nPGCY95gN3B2GNgEwJCsNgA1lleHapYhIpxDWROGcex/Y1UKVi4FHXdAnQHcz6xuOtnVPTaRnWiIb\ny6rCsTsRkU4j2uYo+gFbQ5YLvbJDmNlsM8s3s/zS0tJ22fmQrDTWlypRiIiEirZE0WrOufuccxOd\ncxOzs7PbZZtDstPYoEQhInKAaEsURUD/kOVcrywshmSnU1ZZR0VtQ7h2KSIS9aItUcwDrvKOfpoK\nlDvnisO188HehPZG9SpERPaJD+fOzOxJYBqQZWaFwK+BBADn3D3AK8D5QAFQDXwznO0bmr3/yKdx\n/buHc9ciIlErrInCOXfZEdY74IYwNecQA3ulkeiLY3XxXpgQqVaIiESXaBt6iqgEXxyj+mawrLA8\n0k0REYkaShQHGZvbneVF5QQCujigiAgoURzihNxM9tY1snGnJrRFRECJ4hBjczMB+FzDTyIigBLF\nIfKy00lOiGNp4Z5IN0VEJCooURwk3hfHuNzuLNjQ0iWpRES6DiWKZpw+PJuVxRWUVdZFuikiIhGn\nRNGMU/OyAPiwoCzCLRERiTwlimYc3y+T7qkJfLBOiUJERImiGb4445S8LN5bW4pf51OISBenRHEY\nM8b0oXRvHYs2aVJbRLo2JYrDmD4qh5QEHy8u3RbppoiIRJQSxWGkJsZz1qgcXl2+nUZ/INLNERGJ\nGCWKFnx5fD92VdXzzpr2udWqiEhnpETRgjNHZJOTkcSTC7dEuikiIhGjRNGCeF8cX5/Un3fXlFC0\npybSzRERiQgliiP4+qT+xJlx33vrI90UEZGICHuiMLMZZrbGzArM7KZm1l9jZqVmtsR7fCvcbQyV\n2yOVSyf254mFWyjcXR3JpoiIRERYE4WZ+YC7gPOA0cBlZja6mapPO+fGe48HwtnG5nx/eh5mxh2v\nr4l0U0REwi7cPYrJQIFzboNzrh54Crg4zG1os76ZKXz79CHMXbKNTzbsjHRzRETCKtyJoh+wNWS5\n0Cs72NfMbJmZ/dPM+oenaS377rQ8cnukcPMLy2nQeRUi0oVE42T2i8Ag59xY4E1gTnOVzGy2meWb\nWX5pacef55CS6OPXXxrD2h2V/ElDUCLShYQ7URQBoT2EXK9sH+fcTudc040gHgBOam5Dzrn7nHMT\nnXMTs7OzO6SxBzt7dG+uOnkg972/gad0boWIdBHhThSLgGFmNtjMEoGZwLzQCmbWN2TxImBVGNt3\nRDdfOJrTh2fzq7nLeWdNSaSbIyLS4cKaKJxzjcD3gNcJJoBnnHMrzOw3ZnaRV+37ZrbCzJYC3weu\nCWcbjyTeF8f/XT6BEX0y+M7ji3VzIxGJeeZc57/fwsSJE11+fn5Y91lWWccV9y9gQ1kld1w6jovH\nNzcnLyISvcxssXNu4pHqReNkdqeQlZ7EM9efzEkDe/CDp5bwl7fW6mgoEYlJShTHIDMlgTnXTuYr\nE/rxl7fWcd5fP+AjDUWJSIxRojhGSfE+7vz6eB68eiL1jQEuf2ABNzzxKcXluoigiMSG+Eg3IFZM\nH9WbU/KyuO/9Ddz1TgFvryph1qmDmTm5P7k9UiPdPBGRo6bJ7A6wdVc1f3hlFa+t2A7AacOymTmp\nP2eOyCEl0Rfh1omIBLV2MluJogMV7anhmUVbeTZ/K9vKa0mMj2PK4J6cMTybM4Znk5eTjplFupki\n0kUpUUQRf8Dx8fqdvLumhPfWlrKupBKAvpnJnDE8m1OHZXFCv0z690glLk6JQ0TCQ4kiim3bU8P7\na0t5b20p/15Xxt66RgDSk+IZ1TeD0X27Mfq4bozq243BWWlkJCdEuMUiEouUKDqJRn+AlcUVrNxW\nse95VXEFVfX+fXWy0hMZ1CuNQVlpDOqVSm6PVHIyksjplkR2RjLdkuM1hCUibdbaRKGjniIs3hfH\n2NzujM3tvq8sEHBs2VXN6u0VbCyrZlNZFZt2VvHBulL+ubjukG0kJ8TRu1syvTOSye6WRE5GElnp\nSfRKS6RHWiI9UhPJTEmgW0o83ZITSE30KbGISKspUUShuDgL9h6y0g5ZV13fSHF5LSUVdZTsDT7v\nqKilZG8d2ytqWbWtgvf21lHpDWc1xxdnpCb4SE3ykZoYT0qCj7QkHymJ8aQl+khJ9JGa6CMtMZ6U\n0OckHykJ8cF13uvg+4LbSU3waY5FJAYpUXQyqYnxDM1OZ2h2eov1ahv87KqqZ1dVPbur69lb20hF\nTQPlNQ1U1DZQXe+npt5PVb2fmvpGquv9lNc0sL28hqo6PzUNfqrqGqlrbNtlSZIT4vYlltSmBJLo\nI8EXR4LPiI+LIyE+joQ4I8EXR7zP9q/zhZYHyxJ8ccTFGXEGcRb6bMTFBV+bHbge9i+b4T0MY39Z\nnLdsocvGAe9v2q4Rsh2C+7V9dZrfNhy4zkK3E9xF67VhdLitA8ltGXl2bdh627bbhrptHCoPrd70\nuimO/cv7t+sOed/+ugevO2Q77tCy5vYdut+DY3LNvIdm1oau690tmf49O/ZcLSWKGJWc4OO47ikc\n1z3lmLbjDzhqGvxU1wWTSVV94wEJpqrOT3XD/tdNCabGqxuakBoaAzQGAjT4HQ3+AI3ec4M/QGOg\n6XXnnzMTCafrzxjKTeeN7NB9KFFIi3xxRnpSPOlJ4fmqOOdoDLhgEgkEaGgMEHDB8oADv3MEAsH/\nygLOeY/ger/bXx763PQfY8ABNNUPWe+C/wce8D7vPcHlA9+/r4z9+276bzIQ2P9+3P7/IIPbO3C5\nLZ2KtkwpWZu23NZtd9R2O6gRXvWmObmmtza1bd+z19s74H3NvKepnfvfd+B2CNnO/nXNbCdkvwfH\nFNqMg+cSD1wXfA7HlR+UKCSqmJk35AQp6Cx2kWigiwKKiEiLlChERKRFMXHCnZmVApuP8u1ZQFe7\niYRi7hoUc9dwLDEPdM5lH6lSTCSKY2Fm+a05MzGWKOauQTF3DeGIWUNPIiLSIiUKERFpkRIF3Bfp\nBkSAYu4aFHPX0OExd/k5ChERaZl6FCIi0iIlChERaVGXThRmNsPM1phZgZndFOn2tBcze8jMSsxs\neUhZTzN708zWec89vHIzs795P4NlZnZi5Fp+9Mysv5m9Y2YrzWyFmf3AK4/ZuM0s2cwWmtlSL+Zb\nvfLBZrbAi+1pM0v0ypO85QJv/aBItv9omZnPzD4zs5e85ZiOF8DMNpnZ52a2xMzyvbKwfbe7bKIw\nMx9wF3AeMBq4zMxGR7ZV7eYRYMZBZTcB851zw4D53jIE4x/mPWYDd4epje2tEfixc240MBW4wfs8\nYznuOuAs59w4YDwww8ymArcDdzrn8oDdwCyv/ixgt1d+p1evM/oBsCpkOdbjbXKmc258yDkT4ftu\nB6982fUewMnA6yHLPwd+Hul2tWN8g4DlIctrgL7e677AGu/1vcBlzdXrzA/gBeDsrhI3kAp8Ckwh\neJZuvFe+73sOvA6c7L2O9+pZpNvexjhzvT+KZwEvEbygaszGGxL3JiDroLKwfbe7bI8C6AdsDVku\n9MpiVW/nXLH3ejvQ23sdcz8Hb4hhArCAGI/bG4ZZApQAbwLrgT3OuaZbHIbGtS9mb3050Cu8LT5m\nfwF+CjTdUasXsR1vEwe8YWaLzWy2Vxa277YuM94FOeecmcXkcdFmlg48B/yXc64i9Hr+sRi3c84P\njDez7sC/gI69g00EmdmFQIlzbrGZTYt0e8LsVOdckZnlAG+a2erQlR393e7KPYoioH/Icq5XFqt2\nmFlfAO+5xCuPmZ+DmSUQTBL/cM497xXHfNwAzrk9wDsEh166m1nTP4Ghce2L2VufCewMc1OPxSnA\nRWa2CXiK4PDTX4ndePdxzhV5zyUE/yGYTBi/2105USwChnlHTCQCM4F5EW5TR5oHXO29vprgGH5T\n+VXekRJTgfKQ7mynYcGuw4PAKufcn0NWxWzcZpbt9SQwsxSCczKrCCaMS7xqB8fc9LO4BHjbeYPY\nnYFz7ufOuVzn3CCCv69vO+euIEbjbWJmaWaW0fQaOAdYTji/25GepInwBNH5wFqC47q/jHR72jGu\nJ4FioIHg+OQsgmOz84F1wFtAT6+uETz6az3wOTAx0u0/yphPJTiOuwxY4j3Oj+W4gbHAZ17My4Gb\nvfIhwEKgAHgWSPLKk73lAm/9kEjHcAyxTwNe6grxevEt9R4rmv5WhfO7rUt4iIhIi7ry0JOIiLSC\nEoWIiLRIiUJERFoUE+dRZGVluUGDBkW6GSIincrixYvLXCvumR0TiWLQoEHk5+dHuhkiIp2KmW1u\nTT0NPYmISIuUKEREIqSm3s+SrXuobfBHuiktiomhJxGRzmZTWRXffmwxa3bsJSk+jilDenH6sCxO\nH57NsJx0Qq9TFmlKFCIiYfbO6hJ+8NRnxMUZv/3y8WworeT9taX87uVV8PIqjstMZvbpQ7hsygCS\n4n2Rbq4ShYhIuAQCjv97p4A731rLqD7duPcbJ9G/Z+q+9UV7avj3ulKe/7SIW15cyf0fbORHZw/n\nyxP64YuLXA8jJi7hMXHiRKejnkQkmlXUNvCjp5fy1qodfGVCP/7wlRNISWy+t+Cc44N1Zfzx9dUs\nL6pgeO90fnT2CKaNyCY5of16GGa22O2/Y95hqUchItLBCkr2MvvRxWzeVc2vvzSaa74wqMU5CDPj\n9OHZnJqXxavLt/M/b6zh+scXE2cwoGcqeTnpDM1JJy87nRMH9mBodnqHtl+JQkSkA722vJgfP7OU\nlEQfT3xrClOGtP4me3FxxgVj+3LumN68taqElcUVrC+ppKCkkvfWltLgd1x/xlBuOq9j71elRCEi\n0gH8Acf/vLGGv7+7nvH9u3P3lSfSNzPlqLYV74tjxvF9mHF8n31ljf4AW3ZVt+tQ1GH33+F7EBHp\nYvZU1/P9p5bw/tpSLps8gFsuGt3uRy/F++IY0sFDTvv2FZa9iIh0AQ3+AE8t3MJf56+joqaR2756\nAjMnD4h0s46ZEoWIyDFyzvHy58Xc8foaNu2sZvLgntx84WiO75cZ6aa1CyUKEZFjsGDDTn7/yiqW\nFZYzsk8GD18ziWkjsqPqzOpjpUQhInIUnHM89OEmfvfySo7LTOF/Lh0X8RPjOooShYhIGzX6A9zy\n4goe/2QLM8b04c9fH0dqYuz+OY3dyEREjlIg4Fi+rZzcHqn0TEs8YF1FbQM3/ONTPlhXxvVnDOWn\n544gLgZ7EaGUKEREQjjnuPXFFcz5OHhPn9F9u3FKXi9OycviuO4p3PCPT9lYVsXtXzuBr0/q/Ec0\ntUar7kdhZjPMbI2ZFZjZTc2sP93MPjWzRjO75KB1A8zsDTNbZWYrzWyQVz7YzBZ423zazBK98iRv\nucBbP+hYgxQRaa073ljDnI83c+XUAdx47ggyUxKY89Fmrnl4Eefc+T47Kmp59NrJXSZJQCt6FGbm\nA+4CzgYKgUVmNs85tzKk2hbgGuAnzWziUeD3zrk3zSwdCHjltwN3OueeMrN7gFnA3d7zbudcnpnN\n9Op9/aiiExFpg7+/W8Bd76zniikD+O3Fx2Nm3HBmHjX1fvI372JZYTnnHd8nbCe6RYvW9CgmAwXO\nuQ3OuXrgKeDi0ArOuU3OuWXsTwIAmNloIN4596ZXr9I5V23B48bOAv7pVZ0DfNl7fbG3jLd+usXS\ncWYiEpUe+3gTf3xtDV8ef9y+JNEkJdHHacOyueHMvC6XJKB1cxT9gK0hy4XAlFZufziwx8yeBwYD\nbwE3AT2APc65xpBt9jt4f865RjMrB3oBZa3cp4jIIZxzPLlwK++uKWFIdjoj+2QwvHcGQ7LTeHlZ\nMf/9wgrOHt2bP106LuYnp9uqoyez44HTgAkEh6eeJjhE9cKxbtjMZgOzAQYM6DpjhSLSdhW1Ddz0\n3DJe+Xw7fTOTeWdNCQ3+4L14fHFGwDlOzcvify+bQIKvVVO3XUprEkUR0D9kOdcra41CYIlzbgOA\nmc0FpgIPAd3NLN7rVYRus2l/hWYWD2QCOw/esHPuPuA+CN64qJXtEZEuZlnhHr73xGcU7anhpvNG\nMvu0IfidY1NZFWt27GXt9r00BBz/eVZeWK7E2hm1JlEsAoaZ2WCCf8RnApe3cvuLCCaEbOdcKcF5\niXznnDOzd4BLCM55XM3+XsY8b/ljb/3bLhZuwyciYeWc45GPNvGHV1aRnZ7EM9+eykkDewIQhzGs\ndwbDemfA2Ag3tBM4Yh/L+4//e8DrwCrgGefcCjP7jZldBGBmk8ysELgUuNfMVnjv9RM8Emq+mX0O\nGHC/t+mfAT8yswKCcxAPeuUPAr288h8RnNMQEWm1Rn+AHz2zlFtfXMkZw7N5+fun7UsS0na6Z7aI\nxBR/wHHjs0t5/rMifnT2cP7zrLyYukBfe9I9s0WkywkEHDc9t4znPyviJ+cM53tnDYt0k2KCEoVI\nDKip97OyuJzPC8sZmJXGmSNyIt2ksHPO8cu5y3l2cSHfnz5MSaIdKVGIdEK1DX5eXV7Mgg27WFpY\nztode/EH9g8jnzO6N7dePOao79Hc2Tjn+PW8FTy5cAvfnTaUH35RSaI9KVGIRImaej/bymvYXVXP\n4Kw0eqUnHVKnaE8Nj328macXbWF3dQPdUxM4oV8m00cOZWxuJmP6ZfLi0m385a21nP3n97nx3BFc\nOXVgxO6RUFPvZ2NZFbur69lZVc/uquBzz9QEvnHyoHZplz/g+O1LK3n0481cd9pgbjx3hOYk2pkm\ns0UioL4xwItLt/Haiu0U7a5hW3kNe6obDqhzXGYyY/plcvxxmQzslcqry4t5c+UOAM4Z3YervjCQ\nk4f0avaP4pad1fxy7ud8sK6M8f2784Ppw0hN9BHvM+LM8MUZPVIT6d8ztd1jKy6vYf6qEt5eXcKH\nBWXUNQaarffFUb3522XjD3sfh721Dcz9rIizR/ehT2Zys3XKqxv4z6c+4/21pcw6dTC/umCUkkQb\ntHYyW4lCJIz2VNfzjwVbmPPRJkr21tG/Zwp52ekc1z3FeyTTPSWRgpJKlm8rZ3lRORvKqnAOeqQm\nMHPyAK6cOpB+3Y88pOSc44Ul2/jtSyvZWVXfbJ3Jg3py5ckDmTGmD4nxBx4tX17dwDtrSliwcSfD\ne2fwxVG9m00s/oBjydY9vLumhPmrSlhZXAFA/54pTB/Zm4mDetArLYle6Yn0TEuke0oCTyzcwi3z\nVnBCv0weuHoS2RkH9p7eX1vKTc8tY1t5LamJPm44M49Zpw4+4IS4tTv2ct2j+WzbU8MtF43hiikD\nj/gzkQMpUYhEkeLyGu55dz3P5BdS0+DntGFZzDp1MGcMP/K9lavqGtlYVkVeTvpRnTlcXtPAym0V\nBJzDH3D4nSMQcKwrqeSJBVvYsquarPQkZk7qz7lj+vDplt28sXI7CzbsojHgSEv0UVXvB2BYTjrT\nR/Vm2ohsistreGd1Ke+vK2VPdQNxBicN7MH0Ub2ZPjKHvJz0FmN7c+UO/vPJT8nOSOKRb05maHY6\ne2sb+P3Lq3hq0VaGZqfxsxkjee7TQl5fsYOBvVL57wtGM31UDq+v2MGPn1lCalI8d19xIhMH6RyJ\no6FEIRIFnHM8u7iQ3764ktpGPxeN68e3ThvMqL7dIt00IHg46fvrSnn8k83MX11C05+DodlpnDOm\nD2eP7s343O5s2VXN/NUlzF+1g4UbgwkEICs9iTOGZzNtRDanDcuie2piC3s71NKte5g1ZxENfscP\npg/jgQ82sL2ilutOH8IPvzh8X2L8YF0pt764koKSSo7v143lRRWM69+de6886bDDUnJkShQiEVZS\nUcvPn/+c+atLmDy4J3dcMo4Bvdp/TqC9FO6u5uP1OzlpYI8WL6VdUdvAJ+t30jczhTHHdTvmK61u\n3VXN1Q8vZENpFUOz07jj0nFMGNDjkHoN/gCPfryZv81fx4wxfbj14jG6NtMxUqIQiRDnHC8uK+bm\nF5ZTU+/npzNG8s0vDNKlq1tQXt3A22t2cN7xfY/4x985pwnrdqIzs0XCzB9wvLe2hIc/3LTvaKP/\n+Y9xDO2CN7ppq8zUBL4yIbdVdZUkwk+JQuQY7ays4+n8rfzjky0U7akhOyOJX5w/kmtPGUy87m0g\nMUCJQuQo1Tb4+e1LK3k2v5B6f4CpQ3ryi/NHcc6Y3rr5jcQUJQqRo7Crqp7rHs1n8ebdXDFlANd8\nYVDw3gYiMUiJQqSNNpRW8s1HFlFcXstdl5/IBWP7RrpJIh1KiUKkDRZu3MXsx/KJM+PJ66Zy0sBD\nD+MUiTVKFNKl7ays4/lPi9hVXc/e2gYqaxvZW9tIdb2f7qkJZGckkZWeRHZGElV1jfzxtTXk9kzh\n4WsmMbBXWqSbLxIWShTSZX1eWM63H8tnW3kt8XFGRnI8GckJZCTHk5LgY11JJR9v2HnAxfqmDO7J\nvd84qc1nIIt0ZkoU0iXN/ayInz23jF5picz73imc0C/zsMfn1zX62VlZT3lNA8Ny0nXIq3Q5ShTS\npTT6A9z26moe+PdGJg/uyd+vOJGsZu77ECop3rfv6q4iXZEShXQZpXvr+OHTS/h3QRlXnzyQX104\nWuc7iLRCq35LzGyGma0xswIzu6mZ9aeb2adm1mhmlxy0zm9mS7zHvJDys7z3LDezOWYW75VPM7Py\nkPfcfKxBSte2dVc1N7+wnFNvf5uFG3fxx0vGcuvFxytJiLTSEXsUZuYD7gLOBgqBRWY2zzm3MqTa\nFuAa4CfNbKLGOTf+oG3GAXOA6c65tWb2G+Bq4EGvygfOuQvbGoxIqDXb93LPe+uZt3QbcQZfmdCP\n688Y2uKVUUXkUK0ZepoMFDjnNgCY2VPAxcC+ROGc2+Sta/6eh4fqBdQ759Z6y28CP2d/ohA5auXV\nDdzy4gr+9VkRqYk+rvnCIL512mD6ZmqOQeRotCZR9AO2hiwXAlPasI9kM8sHGoHbnHNzgTIg3swm\nOufygUuA/iHvOdnMlgLbgJ8451YcvFEzmw3MBhgwYEAbmiOx7KOCMn787FJK99bx3WlDue60IfRI\n06GsIsciHJPZA51zRWY2BHjbzD53zq03s5nAnWaWBLwB+L36n3rvqTSz84G5wLCDN+qcuw+4D4L3\nowhDHBLF6hr93PH6Gu7/YCNDstJ4/rtfYGxu90g3SyQmtCZRFHHgf/u5XlmrOOeKvOcNZvYuMAFY\n75z7GDgNwMzOAYZ79SpC3vuKmf3dzLKcc2Wt3ad0LSu2lfPjZ5ayevterpw6gF+cP4rURB3QJ9Je\nWvPbtAgYZmaDCSaImcDlrdm4mfUAqp1zdWaWBZwC/NFbl+OcK/F6FD8Dfu+V9wF2OOecmU0meGTW\nzjbGJTHOOcfH63dy/wcbeGdNKVnpiTx0zUTOGtk70k0TiTlHTBTOuUYz+x7wOuADHnLOrfCOVMp3\nzs0zs0nAv4AewJfM7Fbn3BhgFHCvN8kdR3COomkS/EYzu9Arv9s597ZXfgnwHTNrBGqAmS4W7tcq\n7aK+McBLy7bxwAcbWVlcQVZ6Ij/84nCuOnmg5iJEOojumS2dxkfry7jx2WUU7alhWE463zptMBeP\n73fEeyyDzVCjAAAORElEQVSLSPN0z2yJGU0T1Q/8eyODe6Xx8DcnMW14tu6dLBImShQS1VZvr+C/\nnlrC6u17uWLKAH55gSaqRcJNv3ESlZxzPPThJm5/bTXdkuM1US0SQUoUEpXufX8Dt726mi+O6s1t\nXzvhiFd4FZGOo0QhUeeNFdu5/bXVXDi2L/972QTNRYhEmC6fKYdV1+jn3vfW81FBGeE6Om7FtnL+\n6+kljO2XyR2XjlOSEIkC6lFIs+oa/Xzn8U95e3UJAEOz07hy6kC+emIumSkJHbLPkr21XDcnn27J\nCdx/1UQd9ioSJdSjkEOEJolbvjSaP//HOLqlJHDriyuZ+of53PTcMrbuqm7XfdY2+Jn96GJ2Vzfw\nwNUTyemW3K7bF5Gjpx6FHCA0Sfzuy8dz5dSBAHz1xFyWF5Xz+CebmbukiNdXbOe+qyYyaVDPY95n\nIOD46T+XsWTrHu658kSO75d5zNsUkfajM7Nln8MliYNtLKti1iOLKNxdw+2XnMBXJuQe1f5WFVcw\nd0kRLy7ZxrbyWm48dwQ3nJl3LCGISBvozGxpFeccZZX1FJRUcu/763l3TWmLSQJgsHcZ7+88/ik/\nfHopG0ur+OHZw1s18Vxe08ATC7Yw97Mi1uzYiy/OOH1YFr+4YBQXnNC3PUMTkXaiRBFD/IFg79AX\n1/If7M07q7jnvQ2s27GXdSWVlNc0AGDGEZNEk+6picy5djK/mvs5f3u7gI07q/nTJWNbnIDeUFrJ\nrDn5bCyr4sQB3fnNxWO44IS+9NI5EiJRTYkiRuyqqucbDy4g4OCxWZMPe4JacXkNl9+/gN3V9Rzf\nL5MLxvYlLzudvJx0RvbJaNMkcmJ8HLd/bSyDs9K5/bXVrN2+l1suGsPJQ3sdUvfDgjK+8/hiEnxx\nPHv9ye0ytyEi4aE5ihiwu6qeyx9YwIbSSsxgYM80/nHdlEOSxe6qev7j3o8pLq/lqdlT23XS+O3V\nO/jvuSso2lPDBWP78ovzR9Gve/Ae1f9YsJmbX1jB0Ow0Hrx6Ev17prbbfkXk6LV2jkKJopPbU13P\nFQ8sYF1JJQ9cNZH4OOPaOYsOSRbV9Y1c8cACVmyrYM43Jzf7X/+xqm3wc+97G7j7vQIAvjstj93V\n9Tz84SbOHJHN3y6bQEZyx5yDISJtp0TRBZRXN3DFg5+wdnsl9111EtNG5ADwUUHZAcmiW3IC33o0\nn3+vK+XvV5zEjOP7dGi7ivbU8IdXVvHysmIArj1lML+8YNQR505EJLyUKGJceU0D33hwAauL93Lv\nN07izJE5B6xvShYDeqYyLCeDlz8v5ravnsDMyQPC1saFG3exu7qec8d0bGISkaOjw2Nj2IINO/n1\nvBWsL63knisPTRIAX8jL4qGrJ3HtnEWs3VHJjeeOCGuSAJg8WBPWIrFAiSLKbCit5LjuKc0eZlpQ\nUsltr67mrVU76JuZzH1XTeTMEYcmiSZfyMviyeumsm5HJZdOPLqT4kRElCiiyNzPivivp5cQH2eM\n6ZfJxIE9mDiwB3k56cz5eBNPLtxKSoKPG88dwaxTB7fqonkTBvRgwoAeHd94EYlZrUoUZjYD+Cvg\nAx5wzt120PrTgb8AY4GZzrl/hqzzA597i1uccxd55WcBdwCJwGJglnOu0YKn9/4VOB+oBq5xzn16\n9CF2DtvLa7n5heWM69+dLwztxeLNu3n8k808+O+NAMTHGVdOGcD3pw/TCWoiElZHTBRm5gPuAs4G\nCoFFZjbPObcypNoW4BrgJ81sosY5N/6gbcYBc4Dpzrm1ZvYb4GrgQeA8YJj3mALc7T3HLOccP3tu\nGQ1+x1+/Pp5BWWkA1DcGWLGtnJXFFZw8pBdDstMj3FIR6Ypac5nxyUCBc26Dc64eeAq4OLSCc26T\nc24ZEGjlfnsB9c65td7ym8DXvNcXA4+6oE+A7mYW0xcBemrRVt5bW8rPzx+5L0lA8MznCQN6cMWU\ngUoSIhIxrUkU/YCtIcuFXllrJZtZvpl9YmZf9srKgHgzazos6xKgfzvtr1PZuqua3720klPyenHl\nlCNfY0lEJNzCMZk90DlXZGZDgLfN7HPn3HozmwncaWZJwBuAvy0bNbPZwGyAAQPCe9hnewkEHD95\ndilmxh8vGUecTkgTkSjUmh5FEfv/2wfI9cpaxTlX5D1vAN4FJnjLHzvnTnPOTQbeB5qGoVq1P+fc\nfc65ic65idnZ2a1tTlR5+KNNLNi4i5u/NHrfdZFERKJNaxLFImCYmQ02s0RgJjCvNRs3sx5ejwEz\nywJOAVZ6yznecxLwM+Ae723zgKssaCpQ7pwrbkNMncLq7RX88bXVTB+Zw6Un6RwHEYleR0wUzrlG\n4HvA68Aq4Bnn3Aoz+42ZNR3qOsnMCoFLgXvNbIX39lFAvpktBd4Bbgs5WupGM1sFLANedM697ZW/\nAmwACoD7ge+2R6DRZOW2Ci6/fwGZKQn8v6+e0Kob/oiIRIqu9RRmy4vKufLBBaQk+HjiuqkMDjnK\nSUQknFp7rafWDD3FrOLyGv65uJB1O/buuztcR1qydQ+X3/8JaYnxPPPtk5UkRKRT6NKX8PiwYCc/\neXYpAGmJPo7vl8m4/t0Zc1w3MpLjMTN8ZvjijMT4OMbmZpIUf+TLZjRn8eZdXP3QInqmJfLEdVPI\n7aGb94hI59ClE8VXJvRjXG4mSwvLWVa4h6WF5Tzy4Sbq/c2fNzhxYA8evGYSmSmtu/lOoz/AyuIK\nPl6/k7/OX0fvbsk8cd0U+mbqCCcR6Tw0R3GQ+sYAG8oqqW0IEHCOQMDhDzgKSiu5Zd4K8nIyePTa\nyWRnNH+9pYKSSl5fsZ0FG3exeNMuquqDp4eMy83k/qsmtume1CIiHUn3ozhKifFxjOzT7ZDyKUN6\nMaBnKrMfXcyl93zEY7OmHHDv552Vddz51lqeXLgVf8AxoncGXz0xl8mDezJ5cE96K0GISCelHkUb\nLd68m28+vJDUxHgemzWZ/j1TeeSjTdz1dgHVDX6unDKAG87KIydDiUFEoptuhdqBVm+v4BsPLqTR\nHyA9OZ6tu2qYPjKHn58/irwcXbxPRDoHHR7bgUb26cY/rz+ZzJQE0hLjeXzWFB68ZpKShIjEJM1R\nHKWBvdKY/+NpxBk6s1pEYpoSxTHw6WqvItIFaOhJRERapEQhIiItiomjnsysFNh8lG/PInjHvVih\neKJXLMUCsRVPLMUCrY9noHPuiDf0iYlEcSzMLL81h4d1FoonesVSLBBb8cRSLND+8WjoSUREWqRE\nISIiLVKigPsi3YB2pniiVyzFArEVTyzFAu0cT5efoxARkZapRyEiIi3qEonCzHxm9pmZvXRQ+d/M\nrDJkOcnMnjazAjNbYGaDwt3W1jg4HjN7xMw2mtkS7zHeKzcvxgIzW2ZmJ0a25c1rJh4zs9+b2Voz\nW2Vm3w8pj+p4monlg5DPZZuZzfXKoz4WaDae6Wb2qRfPv80szyuP+t+dZmI5y4tluZnNMbN4rzzq\nPxsz22Rmn3ufQ75X1tPM3jSzdd5zD6/8mOPpEokC+AGwKrTAzCYCPQ6qNwvY7ZzLA+4Ebg9P89rs\nkHiAG51z473HEq/sPGCY95gN3B3GNrbFwfFcA/QHRjrnRgFPeeWdIZ4DYnHOndb0uQAfA897qzpD\nLHDoZ3M3cIUXzxPAr7zyzvC7sy8WM4sD5gAznXPHEzwP62qvXmf5bM70vltNh8HeBMx3zg0D5nvL\n0A7xxHyiMLNc4ALggZAyH/An4KcHVb+Y4JcH4J/AdIuyK/41F08LLgYedUGfAN3NrG+HNrCNDhPP\nd4DfOOcCAM65Eq88quNp6bMxs27AWcBcryiqY4HDxuOApjt7ZQLbvNdR/bvTTCy9gHrn3Fpv+U3g\na97rqP9sDiP0M5gDfDmk/JjiiflEAfyFYEIIvRH294B5zrnig+r2A7YCOOcagXKCX6ho0lw8AL/3\nupV3mlnTfVr3xeMp9MqiSXPxDAW+bmb5ZvaqmQ3zyqM9nsN9NhD8pZ3vnKvwlqM9Fmg+nm8Br5hZ\nIfAN4DavPNp/dw6OpQyI90YWAC4h2IuFzvHZOOANM1tsZrO9st4hf9O2A72918ccT0wnCjO7EChx\nzi0OKTsOuBT434g17Cg1F4/n58BIYBLQE/hZuNt2NFqIJwmo9brU9wMPhb1xbdRCLE0uA54MY5OO\nSQvx/BA43zmXCzwM/DnsjWuj5mJxwcM9ZwJ3mtlCYC/gj1ATj8apzrkTCQ4r3WBmp4eu9OJrt0Na\nY/0y46cAF5nZ+UAywS7zCqAOKPB6xqlmVuCNrRYR/K+i0JvYygR2RqTlzTskHjN73Dl3pbe+zswe\nBn7iLTfF0yTXK4sWzcZD8D+eprH8fxH8gwTRHc9hPxszywImA18JqR/NsUDz8bxMcN5ogVfnaeA1\n73U0/+609HtzGoCZnQMM9+pH+2eDc67Iey4xs38R/H7tMLO+zrlib2ipacj22ONxznWJBzANeKmZ\n8sqQ1zcA93ivZwLPRLrdrYkH6Os9G8Eu9m3e8gXAq175VGBhpNvdynhuA64NKV/UmeI5+LsGXA/M\nOahOp4glNB6C/1iWAcO98lnAc97rTvG7c9D3LMd7TiI4+XtWZ/hsgDQgI+T1R8AMgvOuN3nlNwF/\nbK94Yr1H0VYPAo+ZWQGwi+AXvjP4h5llE/wiLCH4hwngFeB8oACoBr4Zmea12W0EY/ohUElwXBw6\nbzwz2T+W36TTxeKcazSz64DnzCwA7Aau9VZ3xt+dG71hqTjgbufc2155tH82vYF/eSMi8cATzrnX\nzGwR8IyZzSJ4FNd/ePWPOR6dmS0iIi2K6clsERE5dkoUIiLSIiUKERFpkRKFiIi0SIlCRERapEQh\nIiItUqIQEZEWKVGIiEiL/j+2XgUJC5kf2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4fd937b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axarr = plt.subplots(2)\n",
    "axarr[0].plot(xgboost.evals_result_['validation_0']['mlogloss']);\n",
    "axarr[1].plot(list(range(440, 500)), xgboost.evals_result_['validation_0']['mlogloss'][440:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно уже некоторое переобучение ближе к 500 деревьям. Судя по валидации, лучшее число деревьев - 340. Его и будем использовать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy_score': 0.9568253968253968, 'log_loss': 0.15670733694691669}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_score(classifier):\n",
    "    score = {'accuracy_score': accuracy_score(y_pred=classifier.predict(X_test), y_true=y_test), \n",
    "                  'log_loss': log_loss(y_pred=classifier.predict_proba(X_test), y_true=y_test)}\n",
    "    return score\n",
    "    \n",
    "\n",
    "xgboost = XGBClassifier(learning_rate=0.2, n_jobs=4, random_state=12, n_estimators=340)\n",
    "xgboost.fit(X_train, y_train)\n",
    "get_score(xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зафиксируйте выбранное количество деревьев. Настройте параметры деревьев, начиная с самых значимых (`max_depth`, `min_child_weight`, `gamma`, `colsample_bytree`). Более подробно подробно про эти параметры вы можете почитать в документации, указанной выше. Не забывайте, что бустинг, как правило, хорошо работает на деревьях небольшой глубины.\n",
    "\n",
    "Правильно подбирать эти параметры по сетке, но данный перебор был бы чересчур трудоемким. Поэтому подбирайте их последовательно.\n",
    "\n",
    "Считать score на каждом шаге не нужно, сравнивайте только обученные классификаторы. Сохраняйте качество (accuracy и logloss) вашего классификатора после каждого настроенного параметра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm #это я для себя использовал, чтобы знать, сколько считаться будет "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85837985a2ab4844ad79c85dc395da0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 {'accuracy_score': 0.94507936507936507, 'log_loss': 0.19084270969946629}\n",
      "3 {'accuracy_score': 0.9568253968253968, 'log_loss': 0.15670733694691669}\n",
      "4 {'accuracy_score': 0.95523809523809522, 'log_loss': 0.15029818570212156}\n",
      "5 {'accuracy_score': 0.95587301587301587, 'log_loss': 0.15005929065877033}\n",
      "6 {'accuracy_score': 0.95460317460317456, 'log_loss': 0.15702568933670508}\n",
      "7 {'accuracy_score': 0.95174603174603178, 'log_loss': 0.15843215004748121}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for max_depth in tqdm(range(2, 8)):\n",
    "    xgboost = XGBClassifier(n_estimators=340, n_jobs=4, random_state=12, learning_rate=0.2, max_depth=max_depth)\n",
    "    xgboost.fit(X_train, y_train)\n",
    "    print(max_depth, get_score(xgboost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53d42e9236249bcbb586c7bb8fbfc77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 {'accuracy_score': 0.9568253968253968, 'log_loss': 0.15670733694691669}\n",
      "0.0555555555556 {'accuracy_score': 0.95523809523809522, 'log_loss': 0.15884613570892966}\n",
      "0.111111111111 {'accuracy_score': 0.95301587301587298, 'log_loss': 0.15940854689468417}\n",
      "0.166666666667 {'accuracy_score': 0.9514285714285714, 'log_loss': 0.16006861144106457}\n",
      "0.222222222222 {'accuracy_score': 0.9514285714285714, 'log_loss': 0.16278316606119381}\n",
      "0.277777777778 {'accuracy_score': 0.95269841269841271, 'log_loss': 0.16438614319049485}\n",
      "0.333333333333 {'accuracy_score': 0.94857142857142862, 'log_loss': 0.16665523639681318}\n",
      "0.388888888889 {'accuracy_score': 0.9501587301587302, 'log_loss': 0.16930747890631642}\n",
      "0.444444444444 {'accuracy_score': 0.94952380952380955, 'log_loss': 0.17112986567052585}\n",
      "0.5 {'accuracy_score': 0.94984126984126982, 'log_loss': 0.17456053271746391}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for gamma in tqdm(np.linspace(0, 0.5, 10)):\n",
    "    xgboost = XGBClassifier(n_estimators=340, n_jobs=4, random_state=12, \n",
    "                            learning_rate=0.2, max_depth=3, gamma=gamma)\n",
    "    xgboost.fit(X_train, y_train)\n",
    "    print(gamma, get_score(xgboost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba3177eaf814552b627a4875656908d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 {'accuracy_score': 0.95492063492063495, 'log_loss': 0.15467971352770435}\n",
      "0.583333333333 {'accuracy_score': 0.9568253968253968, 'log_loss': 0.15540650666911285}\n",
      "0.666666666667 {'accuracy_score': 0.95936507936507931, 'log_loss': 0.15761704406841048}\n",
      "0.75 {'accuracy_score': 0.95587301587301587, 'log_loss': 0.15265542577120222}\n",
      "0.833333333333 {'accuracy_score': 0.95841269841269838, 'log_loss': 0.15456966715204379}\n",
      "0.916666666667 {'accuracy_score': 0.9555555555555556, 'log_loss': 0.1562331246833176}\n",
      "1.0 {'accuracy_score': 0.9568253968253968, 'log_loss': 0.15670733694691669}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for colsample_bytree in tqdm(np.linspace(0.5, 1, 7)):\n",
    "    xgboost = XGBClassifier(n_estimators=340, n_jobs=4, random_state=12, learning_rate=0.2, \n",
    "                            max_depth=3, colsample_bytree=colsample_bytree)\n",
    "    xgboost.fit(X_train, y_train)\n",
    "    print(colsample_bytree, get_score(xgboost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03c24ad1df8466c8e7274ed68ab4d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'accuracy_score': 0.95936507936507931, 'log_loss': 0.15761704406841048}\n",
      "2 {'accuracy_score': 0.95523809523809522, 'log_loss': 0.15857221079586359}\n",
      "3 {'accuracy_score': 0.9555555555555556, 'log_loss': 0.15710306400822155}\n",
      "4 {'accuracy_score': 0.95206349206349206, 'log_loss': 0.15911568062126843}\n",
      "5 {'accuracy_score': 0.95269841269841271, 'log_loss': 0.15979424175308005}\n",
      "6 {'accuracy_score': 0.95238095238095233, 'log_loss': 0.16264280721341484}\n",
      "7 {'accuracy_score': 0.95301587301587298, 'log_loss': 0.1621557073296665}\n",
      "8 {'accuracy_score': 0.95047619047619047, 'log_loss': 0.16433887742319228}\n",
      "9 {'accuracy_score': 0.9501587301587302, 'log_loss': 0.16458265854307089}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for min_child_weight in tqdm(range(1, 10)):\n",
    "    xgboost = XGBClassifier(n_estimators=340, n_jobs=4, random_state=12, learning_rate=0.2, \n",
    "                            max_depth=3, gamma=0, colsample_bytree=0.66667, min_child_weight=min_child_weight)\n",
    "    xgboost.fit(X_train, y_train)\n",
    "    print(min_child_weight, get_score(xgboost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее таким же образом настройте регуляризацию ($\\lambda, \\alpha$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98b73f352f04ae3ae67e5d639173763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.0 {'accuracy_score': 0.95936507936507931, 'log_loss': 0.15761704406841048}\n",
      "0.25 {'accuracy_score': 0.95619047619047615, 'log_loss': 0.15554624840074893}\n",
      "0.5 {'accuracy_score': 0.95396825396825402, 'log_loss': 0.15496458314982081}\n",
      "0.75 {'accuracy_score': 0.95206349206349206, 'log_loss': 0.15727239694611575}\n",
      "1.0 {'accuracy_score': 0.95301587301587298, 'log_loss': 0.15896827074930209}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for reg_alpha in tqdm(np.linspace(0, 1, 5)):\n",
    "    xgboost = XGBClassifier(n_estimators=340, n_jobs=4, random_state=12, learning_rate=0.2, \n",
    "                            max_depth=3, gamma=0, colsample_bytree=0.66667, min_child_weight=1,\n",
    "                            reg_alpha=reg_alpha)\n",
    "    xgboost.fit(X_train, y_train)\n",
    "    print(reg_alpha, get_score(xgboost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27c90cdd25448d490e77c4c8e94a345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 {'accuracy_score': 0.95746031746031746, 'log_loss': 0.16389517704864276}\n",
      "0.25 {'accuracy_score': 0.95619047619047615, 'log_loss': 0.15988853721465235}\n",
      "0.5 {'accuracy_score': 0.95587301587301587, 'log_loss': 0.15706865387639093}\n",
      "0.75 {'accuracy_score': 0.95587301587301587, 'log_loss': 0.15730266982507143}\n",
      "1.0 {'accuracy_score': 0.95936507936507931, 'log_loss': 0.15761704406841048}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for reg_lambda in tqdm(np.linspace(0, 1, 5)):\n",
    "    xgboost = XGBClassifier(n_estimators=340, n_jobs=4, random_state=12, learning_rate=0.2, \n",
    "                            max_depth=3, gamma=0, colsample_bytree=0.66667, min_child_weight=1,\n",
    "                            reg_alpha=0, reg_lambda=reg_lambda)\n",
    "    xgboost.fit(X_train, y_train)\n",
    "    print(reg_lambda, get_score(xgboost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как все параметры настроены, уменьшите `learning_rate`, пропорционально увеличив число деревьев. Обучите итоговый классификатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Увы\n",
    "Я попробовал это сделать, но все попытки приводили лишь к ухудшению скора. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте 2 графика:\n",
    "* по оси X отложены этапы настройки классификатора: по умолчанию, после выбора числа деревьев, после настройки каждого гиперпараметра, итоговый классификатор;\n",
    "* по оси Y на первом графике - accuracy на каждом этапе, на втором - logloss.\n",
    "\n",
    "Какой этап дал наиболее существенный прирост качества? Получилось ли у вас поднять качество выше, чем у базовых решений: kNN и Random Forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzwAAAHjCAYAAADmARo9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZlZZ/w+LzDloo8giLwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff02dd711d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = ['1.default',  '2.trees', '3.max_depth', '4.gamma', \n",
    "     '5.colsample_bytree', '6.min_child_weight', '7.reg_alpha', '8.reg_lambda']\n",
    "\n",
    "y_ll = [0.273825460057, 0.15670733694691669, 0.15670733694691669, 0.15670733694691669, 0.15761704406841048, \n",
    "        0.15761704406841048, 0.15761704406841048, 0.15761704406841048]\n",
    "y_acc = [0.92380952381, 0.9568253968253968, 0.9568253968253968, 0.9568253968253968, 0.95936507936507931, \n",
    "         0.95936507936507931, 0.95936507936507931, 0.95936507936507931] \n",
    "\n",
    "f, axarr = plt.subplots(2, figsize=(14,8))\n",
    "axarr[0].scatter(x, y_acc)\n",
    "axarr[0].set_title('Accuracy');\n",
    "axarr[1].scatter(x, y_ll)\n",
    "axarr[1].set_title('Logloss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самым прибыльным шагом был тюнинг количества деревьев. В остальных гиперпараметрах оптимальными оказались либо дефолтные значения, либо относительно близкие к ним. Да, поднять качество по сравнению с хорошо выступившим kNN удалось. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras\n",
    "(6 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Установите библиотеку keras](https://keras.io/#installation). Возможно, вам также придется [установить библиотеку tensorflow](https://www.tensorflow.org/install/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метки классов для keras необходимо задать не номером, а в виде индикаторов, показывающих принадлежность каждому из классов. Ниже приведен код, делающих это."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве базового решения можете взять модель, которая демонстрировалась на лекции по нейронным сетям. \"Соберите\" сеть, обучите ее и сделайте предсказание. В качестве `batch_size` возьмите 1-2% объема обучающей выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9450 samples, validate on 3150 samples\n",
      "Epoch 1/30\n",
      "9450/9450 [==============================] - 3s - loss: 2.2839 - acc: 0.1875 - val_loss: 2.2625 - val_acc: 0.2940\n",
      "Epoch 2/30\n",
      "9450/9450 [==============================] - 0s - loss: 2.2365 - acc: 0.4314 - val_loss: 2.2142 - val_acc: 0.4813\n",
      "Epoch 3/30\n",
      "9450/9450 [==============================] - 0s - loss: 2.1819 - acc: 0.5402 - val_loss: 2.1549 - val_acc: 0.5356\n",
      "Epoch 4/30\n",
      "9450/9450 [==============================] - 0s - loss: 2.1128 - acc: 0.5708 - val_loss: 2.0782 - val_acc: 0.5740\n",
      "Epoch 5/30\n",
      "9450/9450 [==============================] - 0s - loss: 2.0222 - acc: 0.6121 - val_loss: 1.9768 - val_acc: 0.6063\n",
      "Epoch 6/30\n",
      "9450/9450 [==============================] - 0s - loss: 1.9038 - acc: 0.6422 - val_loss: 1.8459 - val_acc: 0.6489\n",
      "Epoch 7/30\n",
      "9450/9450 [==============================] - 0s - loss: 1.7550 - acc: 0.6734 - val_loss: 1.6859 - val_acc: 0.6889\n",
      "Epoch 8/30\n",
      "9450/9450 [==============================] - 0s - loss: 1.5824 - acc: 0.7040 - val_loss: 1.5087 - val_acc: 0.7041\n",
      "Epoch 9/30\n",
      "9450/9450 [==============================] - 0s - loss: 1.4006 - acc: 0.7308 - val_loss: 1.3321 - val_acc: 0.7368\n",
      "Epoch 10/30\n",
      "9450/9450 [==============================] - 0s - loss: 1.2298 - acc: 0.7613 - val_loss: 1.1751 - val_acc: 0.7616\n",
      "Epoch 11/30\n",
      "9450/9450 [==============================] - 0s - loss: 1.0828 - acc: 0.7856 - val_loss: 1.0411 - val_acc: 0.7835\n",
      "Epoch 12/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.9633 - acc: 0.8038 - val_loss: 0.9351 - val_acc: 0.7997\n",
      "Epoch 13/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.8680 - acc: 0.8160 - val_loss: 0.8500 - val_acc: 0.8127\n",
      "Epoch 14/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.7917 - acc: 0.8292 - val_loss: 0.7831 - val_acc: 0.8222\n",
      "Epoch 15/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.7303 - acc: 0.8371 - val_loss: 0.7279 - val_acc: 0.8292\n",
      "Epoch 16/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.6803 - acc: 0.8442 - val_loss: 0.6854 - val_acc: 0.8375\n",
      "Epoch 17/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.6393 - acc: 0.8501 - val_loss: 0.6456 - val_acc: 0.8460\n",
      "Epoch 18/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.6039 - acc: 0.8563 - val_loss: 0.6152 - val_acc: 0.8460\n",
      "Epoch 19/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.5751 - acc: 0.8601 - val_loss: 0.5867 - val_acc: 0.8533\n",
      "Epoch 20/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.5495 - acc: 0.8647 - val_loss: 0.5667 - val_acc: 0.8530\n",
      "Epoch 21/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.5282 - acc: 0.8684 - val_loss: 0.5441 - val_acc: 0.8603\n",
      "Epoch 22/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.5087 - acc: 0.8699 - val_loss: 0.5288 - val_acc: 0.8600\n",
      "Epoch 23/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.4922 - acc: 0.8734 - val_loss: 0.5114 - val_acc: 0.8644\n",
      "Epoch 24/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.4770 - acc: 0.8774 - val_loss: 0.4998 - val_acc: 0.8648\n",
      "Epoch 25/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.4641 - acc: 0.8786 - val_loss: 0.4855 - val_acc: 0.8714\n",
      "Epoch 26/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.4522 - acc: 0.8812 - val_loss: 0.4743 - val_acc: 0.8727\n",
      "Epoch 27/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.4415 - acc: 0.8836 - val_loss: 0.4658 - val_acc: 0.8721\n",
      "Epoch 28/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.4315 - acc: 0.8857 - val_loss: 0.4548 - val_acc: 0.8749\n",
      "Epoch 29/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.4224 - acc: 0.8870 - val_loss: 0.4461 - val_acc: 0.8778\n",
      "Epoch 30/30\n",
      "9450/9450 [==============================] - 0s - loss: 0.4141 - acc: 0.8894 - val_loss: 0.4390 - val_acc: 0.8806\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "l2_reg = 10 ** -6\n",
    "\n",
    "model.add(Dense(\n",
    "    units=512,\n",
    "    kernel_initializer='uniform',\n",
    "    kernel_regularizer=l2(l2_reg),\n",
    "    input_dim=len(X_train[0])\n",
    "))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(\n",
    "    units=512,\n",
    "    kernel_initializer='uniform',\n",
    "    kernel_regularizer=l2(l2_reg)\n",
    "))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(\n",
    "    units=num_classes,\n",
    "    kernel_initializer='uniform',\n",
    "    kernel_regularizer=l2(l2_reg)\n",
    "))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, nesterov=True)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=sgd,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=200,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем подобрать структуру сети. Каких-то четких инструкций здесь нет, это довольно творческий процесс. Попробуйте варьировать следующие параметры:\n",
    "* число Dense слоев (рекомендуется брать не более пяти); не забывайте после каждого Dense слоя добавлять слой активации; помните, что при большом числе слоев обучение может идти медленно и может потребоваться существенное число эпох для того, что наблюдать прирост качества.\n",
    "* количество нейронов на слоях; как правило, сети проектируют так, чтобы слои сужались: сначала шли слои с большим количеством нейронов, потом - с меньшим.\n",
    "* слои Dropout - вы можете добавить эти слои после скрытых слоев активации.\n",
    "\n",
    "Также, как и в случае с xgboost, сохраняйте наилучший результат по качеству на каждом шаге.\n",
    "\n",
    "Составьте 6-7 разных моделей и посмотрите, какое на них будет качество классификации. Сильно ли зависит результат и время обучения от конфигурации модели?\n",
    "\n",
    "Ниже дано описание класса, с помощью которого можно логировать время обучения. Просто передайте в `fit` параметр `callbacks=[time_callback]`. После обучения вы можете получить время на каждой эпохе с помощью `time_callback.times`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/43178668/record-the-computation-time-for-each-epoch-in-keras-during-model-fit\n",
    "import time \n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "        \n",
    "time_callback = TimeHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Начнем с простого "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9450 samples, validate on 3150 samples\n",
      "Epoch 1/100\n",
      "0s - loss: 1.8699 - acc: 0.4794 - val_loss: 1.5015 - val_acc: 0.6857\n",
      "Epoch 2/100\n",
      "0s - loss: 1.2572 - acc: 0.7562 - val_loss: 1.0942 - val_acc: 0.7819\n",
      "Epoch 3/100\n",
      "0s - loss: 0.9545 - acc: 0.8138 - val_loss: 0.8796 - val_acc: 0.8181\n",
      "Epoch 4/100\n",
      "0s - loss: 0.7887 - acc: 0.8338 - val_loss: 0.7552 - val_acc: 0.8289\n",
      "Epoch 5/100\n",
      "0s - loss: 0.6870 - acc: 0.8477 - val_loss: 0.6730 - val_acc: 0.8390\n",
      "Epoch 6/100\n",
      "0s - loss: 0.6180 - acc: 0.8578 - val_loss: 0.6155 - val_acc: 0.8514\n",
      "Epoch 7/100\n",
      "0s - loss: 0.5685 - acc: 0.8661 - val_loss: 0.5730 - val_acc: 0.8597\n",
      "Epoch 8/100\n",
      "0s - loss: 0.5309 - acc: 0.8705 - val_loss: 0.5387 - val_acc: 0.8635\n",
      "Epoch 9/100\n",
      "0s - loss: 0.5012 - acc: 0.8779 - val_loss: 0.5136 - val_acc: 0.8651\n",
      "Epoch 10/100\n",
      "0s - loss: 0.4768 - acc: 0.8830 - val_loss: 0.4909 - val_acc: 0.8705\n",
      "Epoch 11/100\n",
      "0s - loss: 0.4565 - acc: 0.8865 - val_loss: 0.4729 - val_acc: 0.8778\n",
      "Epoch 12/100\n",
      "0s - loss: 0.4396 - acc: 0.8892 - val_loss: 0.4584 - val_acc: 0.8784\n",
      "Epoch 13/100\n",
      "0s - loss: 0.4247 - acc: 0.8919 - val_loss: 0.4452 - val_acc: 0.8813\n",
      "Epoch 14/100\n",
      "0s - loss: 0.4119 - acc: 0.8947 - val_loss: 0.4336 - val_acc: 0.8844\n",
      "Epoch 15/100\n",
      "0s - loss: 0.4004 - acc: 0.8952 - val_loss: 0.4236 - val_acc: 0.8857\n",
      "Epoch 16/100\n",
      "0s - loss: 0.3903 - acc: 0.8971 - val_loss: 0.4147 - val_acc: 0.8892\n",
      "Epoch 17/100\n",
      "0s - loss: 0.3808 - acc: 0.8993 - val_loss: 0.4064 - val_acc: 0.8898\n",
      "Epoch 18/100\n",
      "0s - loss: 0.3727 - acc: 0.9012 - val_loss: 0.3990 - val_acc: 0.8914\n",
      "Epoch 19/100\n",
      "0s - loss: 0.3653 - acc: 0.9040 - val_loss: 0.3924 - val_acc: 0.8949\n",
      "Epoch 20/100\n",
      "0s - loss: 0.3580 - acc: 0.9046 - val_loss: 0.3870 - val_acc: 0.8943\n",
      "Epoch 21/100\n",
      "0s - loss: 0.3515 - acc: 0.9056 - val_loss: 0.3818 - val_acc: 0.8946\n",
      "Epoch 22/100\n",
      "0s - loss: 0.3454 - acc: 0.9070 - val_loss: 0.3766 - val_acc: 0.8968\n",
      "Epoch 23/100\n",
      "0s - loss: 0.3400 - acc: 0.9091 - val_loss: 0.3719 - val_acc: 0.8987\n",
      "Epoch 24/100\n",
      "0s - loss: 0.3348 - acc: 0.9102 - val_loss: 0.3674 - val_acc: 0.8978\n",
      "Epoch 25/100\n",
      "0s - loss: 0.3297 - acc: 0.9103 - val_loss: 0.3628 - val_acc: 0.9006\n",
      "Epoch 26/100\n",
      "0s - loss: 0.3249 - acc: 0.9123 - val_loss: 0.3593 - val_acc: 0.8990\n",
      "Epoch 27/100\n",
      "0s - loss: 0.3204 - acc: 0.9127 - val_loss: 0.3559 - val_acc: 0.8990\n",
      "Epoch 28/100\n",
      "0s - loss: 0.3163 - acc: 0.9128 - val_loss: 0.3528 - val_acc: 0.9019\n",
      "Epoch 29/100\n",
      "0s - loss: 0.3122 - acc: 0.9141 - val_loss: 0.3492 - val_acc: 0.9016\n",
      "Epoch 30/100\n",
      "0s - loss: 0.3085 - acc: 0.9147 - val_loss: 0.3466 - val_acc: 0.9022\n",
      "Epoch 31/100\n",
      "0s - loss: 0.3047 - acc: 0.9171 - val_loss: 0.3438 - val_acc: 0.9044\n",
      "Epoch 32/100\n",
      "0s - loss: 0.3011 - acc: 0.9164 - val_loss: 0.3407 - val_acc: 0.9035\n",
      "Epoch 33/100\n",
      "0s - loss: 0.2976 - acc: 0.9174 - val_loss: 0.3379 - val_acc: 0.9041\n",
      "Epoch 34/100\n",
      "0s - loss: 0.2942 - acc: 0.9196 - val_loss: 0.3357 - val_acc: 0.9048\n",
      "Epoch 35/100\n",
      "0s - loss: 0.2913 - acc: 0.9198 - val_loss: 0.3329 - val_acc: 0.9067\n",
      "Epoch 36/100\n",
      "0s - loss: 0.2883 - acc: 0.9206 - val_loss: 0.3308 - val_acc: 0.9057\n",
      "Epoch 37/100\n",
      "0s - loss: 0.2853 - acc: 0.9205 - val_loss: 0.3289 - val_acc: 0.9048\n",
      "Epoch 38/100\n",
      "0s - loss: 0.2824 - acc: 0.9219 - val_loss: 0.3270 - val_acc: 0.9067\n",
      "Epoch 39/100\n",
      "0s - loss: 0.2794 - acc: 0.9231 - val_loss: 0.3245 - val_acc: 0.9067\n",
      "Epoch 40/100\n",
      "0s - loss: 0.2768 - acc: 0.9240 - val_loss: 0.3232 - val_acc: 0.9079\n",
      "Epoch 41/100\n",
      "0s - loss: 0.2742 - acc: 0.9249 - val_loss: 0.3212 - val_acc: 0.9102\n",
      "Epoch 42/100\n",
      "0s - loss: 0.2716 - acc: 0.9256 - val_loss: 0.3194 - val_acc: 0.9102\n",
      "Epoch 43/100\n",
      "0s - loss: 0.2693 - acc: 0.9259 - val_loss: 0.3166 - val_acc: 0.9102\n",
      "Epoch 44/100\n",
      "0s - loss: 0.2668 - acc: 0.9268 - val_loss: 0.3153 - val_acc: 0.9108\n",
      "Epoch 45/100\n",
      "0s - loss: 0.2645 - acc: 0.9275 - val_loss: 0.3131 - val_acc: 0.9124\n",
      "Epoch 46/100\n",
      "0s - loss: 0.2623 - acc: 0.9280 - val_loss: 0.3124 - val_acc: 0.9130\n",
      "Epoch 47/100\n",
      "0s - loss: 0.2600 - acc: 0.9291 - val_loss: 0.3108 - val_acc: 0.9140\n",
      "Epoch 48/100\n",
      "0s - loss: 0.2579 - acc: 0.9304 - val_loss: 0.3090 - val_acc: 0.9146\n",
      "Epoch 49/100\n",
      "0s - loss: 0.2556 - acc: 0.9296 - val_loss: 0.3075 - val_acc: 0.9133\n",
      "Epoch 50/100\n",
      "0s - loss: 0.2536 - acc: 0.9309 - val_loss: 0.3054 - val_acc: 0.9146\n",
      "Epoch 51/100\n",
      "0s - loss: 0.2514 - acc: 0.9314 - val_loss: 0.3043 - val_acc: 0.9162\n",
      "Epoch 52/100\n",
      "0s - loss: 0.2495 - acc: 0.9308 - val_loss: 0.3030 - val_acc: 0.9175\n",
      "Epoch 53/100\n",
      "0s - loss: 0.2475 - acc: 0.9326 - val_loss: 0.3016 - val_acc: 0.9165\n",
      "Epoch 54/100\n",
      "0s - loss: 0.2455 - acc: 0.9328 - val_loss: 0.3001 - val_acc: 0.9178\n",
      "Epoch 55/100\n",
      "0s - loss: 0.2437 - acc: 0.9330 - val_loss: 0.2989 - val_acc: 0.9178\n",
      "Epoch 56/100\n",
      "0s - loss: 0.2418 - acc: 0.9332 - val_loss: 0.2973 - val_acc: 0.9178\n",
      "Epoch 57/100\n",
      "0s - loss: 0.2400 - acc: 0.9346 - val_loss: 0.2963 - val_acc: 0.9184\n",
      "Epoch 58/100\n",
      "0s - loss: 0.2381 - acc: 0.9343 - val_loss: 0.2947 - val_acc: 0.9197\n",
      "Epoch 59/100\n",
      "0s - loss: 0.2364 - acc: 0.9351 - val_loss: 0.2933 - val_acc: 0.9181\n",
      "Epoch 60/100\n",
      "0s - loss: 0.2346 - acc: 0.9354 - val_loss: 0.2926 - val_acc: 0.9178\n",
      "Epoch 61/100\n",
      "0s - loss: 0.2329 - acc: 0.9347 - val_loss: 0.2916 - val_acc: 0.9206\n",
      "Epoch 62/100\n",
      "0s - loss: 0.2311 - acc: 0.9361 - val_loss: 0.2908 - val_acc: 0.9197\n",
      "Epoch 63/100\n",
      "0s - loss: 0.2295 - acc: 0.9370 - val_loss: 0.2890 - val_acc: 0.9206\n",
      "Epoch 64/100\n",
      "0s - loss: 0.2278 - acc: 0.9379 - val_loss: 0.2885 - val_acc: 0.9194\n",
      "Epoch 65/100\n",
      "0s - loss: 0.2262 - acc: 0.9378 - val_loss: 0.2874 - val_acc: 0.9206\n",
      "Epoch 66/100\n",
      "0s - loss: 0.2246 - acc: 0.9387 - val_loss: 0.2862 - val_acc: 0.9213\n",
      "Epoch 67/100\n",
      "0s - loss: 0.2232 - acc: 0.9386 - val_loss: 0.2847 - val_acc: 0.9222\n",
      "Epoch 68/100\n",
      "0s - loss: 0.2215 - acc: 0.9402 - val_loss: 0.2840 - val_acc: 0.9210\n",
      "Epoch 69/100\n",
      "0s - loss: 0.2201 - acc: 0.9397 - val_loss: 0.2834 - val_acc: 0.9225\n",
      "Epoch 70/100\n",
      "0s - loss: 0.2184 - acc: 0.9414 - val_loss: 0.2817 - val_acc: 0.9213\n",
      "Epoch 71/100\n",
      "0s - loss: 0.2172 - acc: 0.9412 - val_loss: 0.2813 - val_acc: 0.9229\n",
      "Epoch 72/100\n",
      "0s - loss: 0.2156 - acc: 0.9421 - val_loss: 0.2802 - val_acc: 0.9229\n",
      "Epoch 73/100\n",
      "0s - loss: 0.2142 - acc: 0.9420 - val_loss: 0.2791 - val_acc: 0.9225\n",
      "Epoch 74/100\n",
      "0s - loss: 0.2126 - acc: 0.9431 - val_loss: 0.2791 - val_acc: 0.9232\n",
      "Epoch 75/100\n",
      "0s - loss: 0.2112 - acc: 0.9429 - val_loss: 0.2780 - val_acc: 0.9235\n",
      "Epoch 76/100\n",
      "0s - loss: 0.2100 - acc: 0.9441 - val_loss: 0.2763 - val_acc: 0.9235\n",
      "Epoch 77/100\n",
      "0s - loss: 0.2085 - acc: 0.9444 - val_loss: 0.2754 - val_acc: 0.9225\n",
      "Epoch 78/100\n",
      "0s - loss: 0.2071 - acc: 0.9455 - val_loss: 0.2741 - val_acc: 0.9229\n",
      "Epoch 79/100\n",
      "0s - loss: 0.2057 - acc: 0.9455 - val_loss: 0.2736 - val_acc: 0.9244\n",
      "Epoch 80/100\n",
      "0s - loss: 0.2046 - acc: 0.9450 - val_loss: 0.2728 - val_acc: 0.9244\n",
      "Epoch 81/100\n",
      "0s - loss: 0.2032 - acc: 0.9468 - val_loss: 0.2720 - val_acc: 0.9241\n",
      "Epoch 82/100\n",
      "0s - loss: 0.2020 - acc: 0.9469 - val_loss: 0.2711 - val_acc: 0.9248\n",
      "Epoch 83/100\n",
      "0s - loss: 0.2005 - acc: 0.9473 - val_loss: 0.2703 - val_acc: 0.9248\n",
      "Epoch 84/100\n",
      "0s - loss: 0.1991 - acc: 0.9479 - val_loss: 0.2701 - val_acc: 0.9238\n",
      "Epoch 85/100\n",
      "0s - loss: 0.1981 - acc: 0.9475 - val_loss: 0.2687 - val_acc: 0.9260\n",
      "Epoch 86/100\n",
      "0s - loss: 0.1968 - acc: 0.9485 - val_loss: 0.2688 - val_acc: 0.9263\n",
      "Epoch 87/100\n",
      "0s - loss: 0.1955 - acc: 0.9489 - val_loss: 0.2682 - val_acc: 0.9260\n",
      "Epoch 88/100\n",
      "0s - loss: 0.1944 - acc: 0.9498 - val_loss: 0.2664 - val_acc: 0.9263\n",
      "Epoch 89/100\n",
      "0s - loss: 0.1932 - acc: 0.9496 - val_loss: 0.2659 - val_acc: 0.9260\n",
      "Epoch 90/100\n",
      "0s - loss: 0.1919 - acc: 0.9498 - val_loss: 0.2652 - val_acc: 0.9279\n",
      "Epoch 91/100\n",
      "0s - loss: 0.1907 - acc: 0.9504 - val_loss: 0.2644 - val_acc: 0.9283\n",
      "Epoch 92/100\n",
      "0s - loss: 0.1895 - acc: 0.9503 - val_loss: 0.2639 - val_acc: 0.9289\n",
      "Epoch 93/100\n",
      "0s - loss: 0.1884 - acc: 0.9503 - val_loss: 0.2632 - val_acc: 0.9273\n",
      "Epoch 94/100\n",
      "0s - loss: 0.1872 - acc: 0.9512 - val_loss: 0.2634 - val_acc: 0.9276\n",
      "Epoch 95/100\n",
      "0s - loss: 0.1862 - acc: 0.9519 - val_loss: 0.2615 - val_acc: 0.9270\n",
      "Epoch 96/100\n",
      "0s - loss: 0.1850 - acc: 0.9523 - val_loss: 0.2613 - val_acc: 0.9286\n",
      "Epoch 97/100\n",
      "0s - loss: 0.1839 - acc: 0.9521 - val_loss: 0.2598 - val_acc: 0.9292\n",
      "Epoch 98/100\n",
      "0s - loss: 0.1828 - acc: 0.9521 - val_loss: 0.2593 - val_acc: 0.9283\n",
      "Epoch 99/100\n",
      "0s - loss: 0.1817 - acc: 0.9528 - val_loss: 0.2589 - val_acc: 0.9292\n",
      "Epoch 100/100\n",
      "0s - loss: 0.1806 - acc: 0.9530 - val_loss: 0.2583 - val_acc: 0.9302\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(800, input_dim=len(X_train[0]), activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "model.add(Dense(10, activation=\"softmax\", kernel_initializer=\"normal\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=128,\n",
    "                    epochs=100, validation_data=(X_test, y_test),\n",
    "                    verbose=2, callbacks=[time_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Больше слоев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9450 samples, validate on 3150 samples\n",
      "Epoch 1/100\n",
      "0s - loss: 2.2208 - acc: 0.2847 - val_loss: 2.1400 - val_acc: 0.4489\n",
      "Epoch 2/100\n",
      "0s - loss: 2.0239 - acc: 0.5563 - val_loss: 1.9006 - val_acc: 0.6267\n",
      "Epoch 3/100\n",
      "0s - loss: 1.7312 - acc: 0.6764 - val_loss: 1.5731 - val_acc: 0.7286\n",
      "Epoch 4/100\n",
      "0s - loss: 1.3876 - acc: 0.7616 - val_loss: 1.2408 - val_acc: 0.7743\n",
      "Epoch 5/100\n",
      "0s - loss: 1.0886 - acc: 0.8046 - val_loss: 0.9883 - val_acc: 0.8184\n",
      "Epoch 6/100\n",
      "0s - loss: 0.8784 - acc: 0.8303 - val_loss: 0.8188 - val_acc: 0.8321\n",
      "Epoch 7/100\n",
      "0s - loss: 0.7397 - acc: 0.8450 - val_loss: 0.7079 - val_acc: 0.8460\n",
      "Epoch 8/100\n",
      "0s - loss: 0.6462 - acc: 0.8560 - val_loss: 0.6315 - val_acc: 0.8498\n",
      "Epoch 9/100\n",
      "0s - loss: 0.5806 - acc: 0.8634 - val_loss: 0.5768 - val_acc: 0.8568\n",
      "Epoch 10/100\n",
      "0s - loss: 0.5329 - acc: 0.8701 - val_loss: 0.5359 - val_acc: 0.8597\n",
      "Epoch 11/100\n",
      "0s - loss: 0.4959 - acc: 0.8752 - val_loss: 0.5050 - val_acc: 0.8683\n",
      "Epoch 12/100\n",
      "0s - loss: 0.4666 - acc: 0.8795 - val_loss: 0.4825 - val_acc: 0.8644\n",
      "Epoch 13/100\n",
      "0s - loss: 0.4434 - acc: 0.8848 - val_loss: 0.4632 - val_acc: 0.8721\n",
      "Epoch 14/100\n",
      "0s - loss: 0.4238 - acc: 0.8875 - val_loss: 0.4441 - val_acc: 0.8768\n",
      "Epoch 15/100\n",
      "0s - loss: 0.4073 - acc: 0.8902 - val_loss: 0.4288 - val_acc: 0.8825\n",
      "Epoch 16/100\n",
      "0s - loss: 0.3932 - acc: 0.8930 - val_loss: 0.4168 - val_acc: 0.8835\n",
      "Epoch 17/100\n",
      "0s - loss: 0.3809 - acc: 0.8956 - val_loss: 0.4068 - val_acc: 0.8873\n",
      "Epoch 18/100\n",
      "0s - loss: 0.3701 - acc: 0.8986 - val_loss: 0.3969 - val_acc: 0.8886\n",
      "Epoch 19/100\n",
      "0s - loss: 0.3601 - acc: 0.9005 - val_loss: 0.3891 - val_acc: 0.8902\n",
      "Epoch 20/100\n",
      "0s - loss: 0.3507 - acc: 0.9036 - val_loss: 0.3812 - val_acc: 0.8924\n",
      "Epoch 21/100\n",
      "0s - loss: 0.3431 - acc: 0.9060 - val_loss: 0.3750 - val_acc: 0.8937\n",
      "Epoch 22/100\n",
      "0s - loss: 0.3354 - acc: 0.9067 - val_loss: 0.3710 - val_acc: 0.8933\n",
      "Epoch 23/100\n",
      "0s - loss: 0.3283 - acc: 0.9103 - val_loss: 0.3641 - val_acc: 0.8949\n",
      "Epoch 24/100\n",
      "0s - loss: 0.3223 - acc: 0.9107 - val_loss: 0.3581 - val_acc: 0.8971\n",
      "Epoch 25/100\n",
      "0s - loss: 0.3161 - acc: 0.9125 - val_loss: 0.3539 - val_acc: 0.8990\n",
      "Epoch 26/100\n",
      "0s - loss: 0.3104 - acc: 0.9144 - val_loss: 0.3503 - val_acc: 0.8984\n",
      "Epoch 27/100\n",
      "0s - loss: 0.3049 - acc: 0.9148 - val_loss: 0.3478 - val_acc: 0.8984\n",
      "Epoch 28/100\n",
      "0s - loss: 0.3000 - acc: 0.9160 - val_loss: 0.3406 - val_acc: 0.9029\n",
      "Epoch 29/100\n",
      "0s - loss: 0.2948 - acc: 0.9188 - val_loss: 0.3374 - val_acc: 0.9044\n",
      "Epoch 30/100\n",
      "0s - loss: 0.2906 - acc: 0.9188 - val_loss: 0.3342 - val_acc: 0.9054\n",
      "Epoch 31/100\n",
      "0s - loss: 0.2853 - acc: 0.9203 - val_loss: 0.3328 - val_acc: 0.9048\n",
      "Epoch 32/100\n",
      "0s - loss: 0.2817 - acc: 0.9213 - val_loss: 0.3269 - val_acc: 0.9054\n",
      "Epoch 33/100\n",
      "0s - loss: 0.2778 - acc: 0.9231 - val_loss: 0.3245 - val_acc: 0.9063\n",
      "Epoch 34/100\n",
      "0s - loss: 0.2735 - acc: 0.9238 - val_loss: 0.3215 - val_acc: 0.9083\n",
      "Epoch 35/100\n",
      "0s - loss: 0.2698 - acc: 0.9253 - val_loss: 0.3189 - val_acc: 0.9102\n",
      "Epoch 36/100\n",
      "0s - loss: 0.2661 - acc: 0.9259 - val_loss: 0.3168 - val_acc: 0.9108\n",
      "Epoch 37/100\n",
      "0s - loss: 0.2621 - acc: 0.9268 - val_loss: 0.3142 - val_acc: 0.9114\n",
      "Epoch 38/100\n",
      "0s - loss: 0.2589 - acc: 0.9269 - val_loss: 0.3104 - val_acc: 0.9130\n",
      "Epoch 39/100\n",
      "0s - loss: 0.2554 - acc: 0.9283 - val_loss: 0.3096 - val_acc: 0.9137\n",
      "Epoch 40/100\n",
      "0s - loss: 0.2524 - acc: 0.9290 - val_loss: 0.3080 - val_acc: 0.9143\n",
      "Epoch 41/100\n",
      "0s - loss: 0.2490 - acc: 0.9296 - val_loss: 0.3056 - val_acc: 0.9137\n",
      "Epoch 42/100\n",
      "0s - loss: 0.2461 - acc: 0.9296 - val_loss: 0.3024 - val_acc: 0.9156\n",
      "Epoch 43/100\n",
      "0s - loss: 0.2432 - acc: 0.9309 - val_loss: 0.3011 - val_acc: 0.9162\n",
      "Epoch 44/100\n",
      "0s - loss: 0.2399 - acc: 0.9314 - val_loss: 0.2993 - val_acc: 0.9162\n",
      "Epoch 45/100\n",
      "0s - loss: 0.2370 - acc: 0.9331 - val_loss: 0.2979 - val_acc: 0.9156\n",
      "Epoch 46/100\n",
      "0s - loss: 0.2344 - acc: 0.9337 - val_loss: 0.2950 - val_acc: 0.9168\n",
      "Epoch 47/100\n",
      "0s - loss: 0.2313 - acc: 0.9349 - val_loss: 0.2916 - val_acc: 0.9200\n",
      "Epoch 48/100\n",
      "0s - loss: 0.2289 - acc: 0.9344 - val_loss: 0.2897 - val_acc: 0.9197\n",
      "Epoch 49/100\n",
      "0s - loss: 0.2264 - acc: 0.9369 - val_loss: 0.2904 - val_acc: 0.9181\n",
      "Epoch 50/100\n",
      "0s - loss: 0.2231 - acc: 0.9377 - val_loss: 0.2894 - val_acc: 0.9162\n",
      "Epoch 51/100\n",
      "0s - loss: 0.2211 - acc: 0.9374 - val_loss: 0.2852 - val_acc: 0.9213\n",
      "Epoch 52/100\n",
      "0s - loss: 0.2181 - acc: 0.9384 - val_loss: 0.2830 - val_acc: 0.9200\n",
      "Epoch 53/100\n",
      "0s - loss: 0.2160 - acc: 0.9401 - val_loss: 0.2819 - val_acc: 0.9219\n",
      "Epoch 54/100\n",
      "0s - loss: 0.2132 - acc: 0.9396 - val_loss: 0.2792 - val_acc: 0.9229\n",
      "Epoch 55/100\n",
      "0s - loss: 0.2111 - acc: 0.9413 - val_loss: 0.2794 - val_acc: 0.9219\n",
      "Epoch 56/100\n",
      "0s - loss: 0.2083 - acc: 0.9422 - val_loss: 0.2787 - val_acc: 0.9216\n",
      "Epoch 57/100\n",
      "0s - loss: 0.2061 - acc: 0.9432 - val_loss: 0.2764 - val_acc: 0.9222\n",
      "Epoch 58/100\n",
      "0s - loss: 0.2032 - acc: 0.9442 - val_loss: 0.2742 - val_acc: 0.9210\n",
      "Epoch 59/100\n",
      "0s - loss: 0.2016 - acc: 0.9439 - val_loss: 0.2732 - val_acc: 0.9241\n",
      "Epoch 60/100\n",
      "0s - loss: 0.1991 - acc: 0.9439 - val_loss: 0.2731 - val_acc: 0.9225\n",
      "Epoch 61/100\n",
      "0s - loss: 0.1972 - acc: 0.9466 - val_loss: 0.2706 - val_acc: 0.9219\n",
      "Epoch 62/100\n",
      "0s - loss: 0.1948 - acc: 0.9463 - val_loss: 0.2703 - val_acc: 0.9229\n",
      "Epoch 63/100\n",
      "0s - loss: 0.1927 - acc: 0.9470 - val_loss: 0.2685 - val_acc: 0.9235\n",
      "Epoch 64/100\n",
      "0s - loss: 0.1903 - acc: 0.9484 - val_loss: 0.2687 - val_acc: 0.9238\n",
      "Epoch 65/100\n",
      "0s - loss: 0.1882 - acc: 0.9476 - val_loss: 0.2655 - val_acc: 0.9244\n",
      "Epoch 66/100\n",
      "0s - loss: 0.1865 - acc: 0.9497 - val_loss: 0.2659 - val_acc: 0.9235\n",
      "Epoch 67/100\n",
      "0s - loss: 0.1842 - acc: 0.9490 - val_loss: 0.2631 - val_acc: 0.9267\n",
      "Epoch 68/100\n",
      "0s - loss: 0.1824 - acc: 0.9502 - val_loss: 0.2624 - val_acc: 0.9273\n",
      "Epoch 69/100\n",
      "0s - loss: 0.1804 - acc: 0.9510 - val_loss: 0.2612 - val_acc: 0.9289\n",
      "Epoch 70/100\n",
      "0s - loss: 0.1785 - acc: 0.9514 - val_loss: 0.2590 - val_acc: 0.9273\n",
      "Epoch 71/100\n",
      "0s - loss: 0.1765 - acc: 0.9512 - val_loss: 0.2601 - val_acc: 0.9286\n",
      "Epoch 72/100\n",
      "0s - loss: 0.1746 - acc: 0.9529 - val_loss: 0.2589 - val_acc: 0.9251\n",
      "Epoch 73/100\n",
      "0s - loss: 0.1727 - acc: 0.9531 - val_loss: 0.2577 - val_acc: 0.9276\n",
      "Epoch 74/100\n",
      "0s - loss: 0.1707 - acc: 0.9539 - val_loss: 0.2546 - val_acc: 0.9283\n",
      "Epoch 75/100\n",
      "0s - loss: 0.1687 - acc: 0.9548 - val_loss: 0.2561 - val_acc: 0.9267\n",
      "Epoch 76/100\n",
      "0s - loss: 0.1671 - acc: 0.9558 - val_loss: 0.2532 - val_acc: 0.9279\n",
      "Epoch 77/100\n",
      "0s - loss: 0.1653 - acc: 0.9566 - val_loss: 0.2525 - val_acc: 0.9283\n",
      "Epoch 78/100\n",
      "0s - loss: 0.1634 - acc: 0.9566 - val_loss: 0.2518 - val_acc: 0.9305\n",
      "Epoch 79/100\n",
      "0s - loss: 0.1617 - acc: 0.9580 - val_loss: 0.2513 - val_acc: 0.9289\n",
      "Epoch 80/100\n",
      "0s - loss: 0.1601 - acc: 0.9580 - val_loss: 0.2506 - val_acc: 0.9305\n",
      "Epoch 81/100\n",
      "0s - loss: 0.1584 - acc: 0.9584 - val_loss: 0.2489 - val_acc: 0.9317\n",
      "Epoch 82/100\n",
      "0s - loss: 0.1569 - acc: 0.9583 - val_loss: 0.2481 - val_acc: 0.9314\n",
      "Epoch 83/100\n",
      "0s - loss: 0.1549 - acc: 0.9589 - val_loss: 0.2471 - val_acc: 0.9314\n",
      "Epoch 84/100\n",
      "0s - loss: 0.1530 - acc: 0.9604 - val_loss: 0.2488 - val_acc: 0.9279\n",
      "Epoch 85/100\n",
      "0s - loss: 0.1518 - acc: 0.9602 - val_loss: 0.2459 - val_acc: 0.9292\n",
      "Epoch 86/100\n",
      "0s - loss: 0.1501 - acc: 0.9611 - val_loss: 0.2448 - val_acc: 0.9314\n",
      "Epoch 87/100\n",
      "0s - loss: 0.1481 - acc: 0.9616 - val_loss: 0.2459 - val_acc: 0.9311\n",
      "Epoch 88/100\n",
      "0s - loss: 0.1468 - acc: 0.9622 - val_loss: 0.2443 - val_acc: 0.9298\n",
      "Epoch 89/100\n",
      "0s - loss: 0.1450 - acc: 0.9630 - val_loss: 0.2441 - val_acc: 0.9330\n",
      "Epoch 90/100\n",
      "0s - loss: 0.1437 - acc: 0.9628 - val_loss: 0.2415 - val_acc: 0.9311\n",
      "Epoch 91/100\n",
      "0s - loss: 0.1419 - acc: 0.9637 - val_loss: 0.2414 - val_acc: 0.9333\n",
      "Epoch 92/100\n",
      "0s - loss: 0.1405 - acc: 0.9646 - val_loss: 0.2412 - val_acc: 0.9295\n",
      "Epoch 93/100\n",
      "0s - loss: 0.1392 - acc: 0.9639 - val_loss: 0.2400 - val_acc: 0.9321\n",
      "Epoch 94/100\n",
      "0s - loss: 0.1375 - acc: 0.9648 - val_loss: 0.2397 - val_acc: 0.9324\n",
      "Epoch 95/100\n",
      "0s - loss: 0.1363 - acc: 0.9642 - val_loss: 0.2392 - val_acc: 0.9333\n",
      "Epoch 96/100\n",
      "0s - loss: 0.1346 - acc: 0.9655 - val_loss: 0.2384 - val_acc: 0.9308\n",
      "Epoch 97/100\n",
      "0s - loss: 0.1333 - acc: 0.9660 - val_loss: 0.2383 - val_acc: 0.9337\n",
      "Epoch 98/100\n",
      "0s - loss: 0.1317 - acc: 0.9668 - val_loss: 0.2361 - val_acc: 0.9324\n",
      "Epoch 99/100\n",
      "0s - loss: 0.1302 - acc: 0.9675 - val_loss: 0.2357 - val_acc: 0.9330\n",
      "Epoch 100/100\n",
      "0s - loss: 0.1291 - acc: 0.9675 - val_loss: 0.2353 - val_acc: 0.9324\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(800, input_dim=len(X_train[0]), activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "model.add(Dense(70, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "model.add(Dense(10, activation=\"softmax\", kernel_initializer=\"normal\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=128,\n",
    "                    epochs=100, validation_data=(X_test, y_test),\n",
    "                    verbose=2, callbacks=[time_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дропаут"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9450 samples, validate on 3150 samples\n",
      "Epoch 1/100\n",
      "0s - loss: 2.2395 - acc: 0.2106 - val_loss: 2.1631 - val_acc: 0.3806\n",
      "Epoch 2/100\n",
      "0s - loss: 2.0876 - acc: 0.3742 - val_loss: 1.9670 - val_acc: 0.5584\n",
      "Epoch 3/100\n",
      "0s - loss: 1.8602 - acc: 0.5073 - val_loss: 1.6925 - val_acc: 0.6343\n",
      "Epoch 4/100\n",
      "0s - loss: 1.5881 - acc: 0.5927 - val_loss: 1.3985 - val_acc: 0.7048\n",
      "Epoch 5/100\n",
      "0s - loss: 1.3238 - acc: 0.6519 - val_loss: 1.1492 - val_acc: 0.7590\n",
      "Epoch 6/100\n",
      "0s - loss: 1.1447 - acc: 0.6810 - val_loss: 0.9695 - val_acc: 0.8000\n",
      "Epoch 7/100\n",
      "0s - loss: 0.9985 - acc: 0.7178 - val_loss: 0.8426 - val_acc: 0.8165\n",
      "Epoch 8/100\n",
      "0s - loss: 0.9051 - acc: 0.7393 - val_loss: 0.7520 - val_acc: 0.8270\n",
      "Epoch 9/100\n",
      "0s - loss: 0.8193 - acc: 0.7600 - val_loss: 0.6835 - val_acc: 0.8333\n",
      "Epoch 10/100\n",
      "0s - loss: 0.7628 - acc: 0.7722 - val_loss: 0.6324 - val_acc: 0.8394\n",
      "Epoch 11/100\n",
      "0s - loss: 0.7144 - acc: 0.7847 - val_loss: 0.5934 - val_acc: 0.8463\n",
      "Epoch 12/100\n",
      "0s - loss: 0.6738 - acc: 0.8032 - val_loss: 0.5611 - val_acc: 0.8530\n",
      "Epoch 13/100\n",
      "0s - loss: 0.6389 - acc: 0.8090 - val_loss: 0.5320 - val_acc: 0.8629\n",
      "Epoch 14/100\n",
      "0s - loss: 0.6124 - acc: 0.8200 - val_loss: 0.5096 - val_acc: 0.8667\n",
      "Epoch 15/100\n",
      "0s - loss: 0.5942 - acc: 0.8230 - val_loss: 0.4900 - val_acc: 0.8721\n",
      "Epoch 16/100\n",
      "0s - loss: 0.5719 - acc: 0.8296 - val_loss: 0.4739 - val_acc: 0.8749\n",
      "Epoch 17/100\n",
      "0s - loss: 0.5578 - acc: 0.8337 - val_loss: 0.4586 - val_acc: 0.8784\n",
      "Epoch 18/100\n",
      "0s - loss: 0.5454 - acc: 0.8385 - val_loss: 0.4478 - val_acc: 0.8784\n",
      "Epoch 19/100\n",
      "0s - loss: 0.5250 - acc: 0.8444 - val_loss: 0.4360 - val_acc: 0.8810\n",
      "Epoch 20/100\n",
      "0s - loss: 0.5102 - acc: 0.8487 - val_loss: 0.4248 - val_acc: 0.8825\n",
      "Epoch 21/100\n",
      "0s - loss: 0.4929 - acc: 0.8521 - val_loss: 0.4162 - val_acc: 0.8844\n",
      "Epoch 22/100\n",
      "0s - loss: 0.4875 - acc: 0.8547 - val_loss: 0.4078 - val_acc: 0.8873\n",
      "Epoch 23/100\n",
      "0s - loss: 0.4818 - acc: 0.8559 - val_loss: 0.3996 - val_acc: 0.8895\n",
      "Epoch 24/100\n",
      "0s - loss: 0.4689 - acc: 0.8616 - val_loss: 0.3913 - val_acc: 0.8930\n",
      "Epoch 25/100\n",
      "0s - loss: 0.4565 - acc: 0.8657 - val_loss: 0.3848 - val_acc: 0.8930\n",
      "Epoch 26/100\n",
      "0s - loss: 0.4573 - acc: 0.8639 - val_loss: 0.3785 - val_acc: 0.8965\n",
      "Epoch 27/100\n",
      "0s - loss: 0.4388 - acc: 0.8717 - val_loss: 0.3729 - val_acc: 0.9000\n",
      "Epoch 28/100\n",
      "0s - loss: 0.4328 - acc: 0.8728 - val_loss: 0.3666 - val_acc: 0.8981\n",
      "Epoch 29/100\n",
      "0s - loss: 0.4282 - acc: 0.8741 - val_loss: 0.3634 - val_acc: 0.8987\n",
      "Epoch 30/100\n",
      "0s - loss: 0.4117 - acc: 0.8750 - val_loss: 0.3554 - val_acc: 0.9006\n",
      "Epoch 31/100\n",
      "0s - loss: 0.4083 - acc: 0.8775 - val_loss: 0.3534 - val_acc: 0.9013\n",
      "Epoch 32/100\n",
      "0s - loss: 0.4037 - acc: 0.8790 - val_loss: 0.3485 - val_acc: 0.9025\n",
      "Epoch 33/100\n",
      "0s - loss: 0.4004 - acc: 0.8816 - val_loss: 0.3423 - val_acc: 0.9041\n",
      "Epoch 34/100\n",
      "0s - loss: 0.3843 - acc: 0.8863 - val_loss: 0.3384 - val_acc: 0.9060\n",
      "Epoch 35/100\n",
      "0s - loss: 0.3905 - acc: 0.8869 - val_loss: 0.3346 - val_acc: 0.9060\n",
      "Epoch 36/100\n",
      "0s - loss: 0.3821 - acc: 0.8895 - val_loss: 0.3307 - val_acc: 0.9076\n",
      "Epoch 37/100\n",
      "0s - loss: 0.3720 - acc: 0.8927 - val_loss: 0.3280 - val_acc: 0.9079\n",
      "Epoch 38/100\n",
      "0s - loss: 0.3709 - acc: 0.8892 - val_loss: 0.3247 - val_acc: 0.9089\n",
      "Epoch 39/100\n",
      "0s - loss: 0.3646 - acc: 0.8913 - val_loss: 0.3218 - val_acc: 0.9095\n",
      "Epoch 40/100\n",
      "0s - loss: 0.3646 - acc: 0.8933 - val_loss: 0.3188 - val_acc: 0.9114\n",
      "Epoch 41/100\n",
      "0s - loss: 0.3596 - acc: 0.8924 - val_loss: 0.3152 - val_acc: 0.9121\n",
      "Epoch 42/100\n",
      "0s - loss: 0.3514 - acc: 0.8962 - val_loss: 0.3123 - val_acc: 0.9130\n",
      "Epoch 43/100\n",
      "0s - loss: 0.3560 - acc: 0.8947 - val_loss: 0.3105 - val_acc: 0.9133\n",
      "Epoch 44/100\n",
      "0s - loss: 0.3422 - acc: 0.8980 - val_loss: 0.3087 - val_acc: 0.9133\n",
      "Epoch 45/100\n",
      "0s - loss: 0.3486 - acc: 0.8988 - val_loss: 0.3046 - val_acc: 0.9130\n",
      "Epoch 46/100\n",
      "0s - loss: 0.3438 - acc: 0.8992 - val_loss: 0.3019 - val_acc: 0.9159\n",
      "Epoch 47/100\n",
      "0s - loss: 0.3386 - acc: 0.8980 - val_loss: 0.2995 - val_acc: 0.9159\n",
      "Epoch 48/100\n",
      "0s - loss: 0.3330 - acc: 0.8996 - val_loss: 0.2967 - val_acc: 0.9159\n",
      "Epoch 49/100\n",
      "0s - loss: 0.3292 - acc: 0.9038 - val_loss: 0.2949 - val_acc: 0.9165\n",
      "Epoch 50/100\n",
      "0s - loss: 0.3254 - acc: 0.9037 - val_loss: 0.2925 - val_acc: 0.9171\n",
      "Epoch 51/100\n",
      "0s - loss: 0.3203 - acc: 0.9061 - val_loss: 0.2890 - val_acc: 0.9187\n",
      "Epoch 52/100\n",
      "0s - loss: 0.3203 - acc: 0.9030 - val_loss: 0.2885 - val_acc: 0.9187\n",
      "Epoch 53/100\n",
      "0s - loss: 0.3165 - acc: 0.9061 - val_loss: 0.2848 - val_acc: 0.9203\n",
      "Epoch 54/100\n",
      "0s - loss: 0.3100 - acc: 0.9104 - val_loss: 0.2831 - val_acc: 0.9197\n",
      "Epoch 55/100\n",
      "0s - loss: 0.3091 - acc: 0.9083 - val_loss: 0.2808 - val_acc: 0.9213\n",
      "Epoch 56/100\n",
      "0s - loss: 0.3064 - acc: 0.9103 - val_loss: 0.2795 - val_acc: 0.9222\n",
      "Epoch 57/100\n",
      "0s - loss: 0.3098 - acc: 0.9057 - val_loss: 0.2780 - val_acc: 0.9229\n",
      "Epoch 58/100\n",
      "0s - loss: 0.2998 - acc: 0.9112 - val_loss: 0.2757 - val_acc: 0.9244\n",
      "Epoch 59/100\n",
      "0s - loss: 0.3003 - acc: 0.9092 - val_loss: 0.2754 - val_acc: 0.9251\n",
      "Epoch 60/100\n",
      "0s - loss: 0.2962 - acc: 0.9128 - val_loss: 0.2721 - val_acc: 0.9257\n",
      "Epoch 61/100\n",
      "0s - loss: 0.2873 - acc: 0.9141 - val_loss: 0.2697 - val_acc: 0.9251\n",
      "Epoch 62/100\n",
      "0s - loss: 0.2882 - acc: 0.9143 - val_loss: 0.2677 - val_acc: 0.9254\n",
      "Epoch 63/100\n",
      "0s - loss: 0.2825 - acc: 0.9152 - val_loss: 0.2665 - val_acc: 0.9257\n",
      "Epoch 64/100\n",
      "0s - loss: 0.2869 - acc: 0.9129 - val_loss: 0.2659 - val_acc: 0.9254\n",
      "Epoch 65/100\n",
      "0s - loss: 0.2773 - acc: 0.9198 - val_loss: 0.2639 - val_acc: 0.9270\n",
      "Epoch 66/100\n",
      "0s - loss: 0.2750 - acc: 0.9213 - val_loss: 0.2617 - val_acc: 0.9257\n",
      "Epoch 67/100\n",
      "0s - loss: 0.2753 - acc: 0.9199 - val_loss: 0.2593 - val_acc: 0.9279\n",
      "Epoch 68/100\n",
      "0s - loss: 0.2689 - acc: 0.9220 - val_loss: 0.2577 - val_acc: 0.9292\n",
      "Epoch 69/100\n",
      "0s - loss: 0.2735 - acc: 0.9208 - val_loss: 0.2567 - val_acc: 0.9292\n",
      "Epoch 70/100\n",
      "0s - loss: 0.2720 - acc: 0.9203 - val_loss: 0.2554 - val_acc: 0.9302\n",
      "Epoch 71/100\n",
      "0s - loss: 0.2654 - acc: 0.9222 - val_loss: 0.2541 - val_acc: 0.9295\n",
      "Epoch 72/100\n",
      "0s - loss: 0.2632 - acc: 0.9226 - val_loss: 0.2528 - val_acc: 0.9305\n",
      "Epoch 73/100\n",
      "0s - loss: 0.2607 - acc: 0.9250 - val_loss: 0.2507 - val_acc: 0.9305\n",
      "Epoch 74/100\n",
      "0s - loss: 0.2597 - acc: 0.9219 - val_loss: 0.2489 - val_acc: 0.9311\n",
      "Epoch 75/100\n",
      "0s - loss: 0.2519 - acc: 0.9267 - val_loss: 0.2481 - val_acc: 0.9311\n",
      "Epoch 76/100\n",
      "0s - loss: 0.2562 - acc: 0.9243 - val_loss: 0.2473 - val_acc: 0.9295\n",
      "Epoch 77/100\n",
      "0s - loss: 0.2500 - acc: 0.9262 - val_loss: 0.2455 - val_acc: 0.9321\n",
      "Epoch 78/100\n",
      "0s - loss: 0.2441 - acc: 0.9287 - val_loss: 0.2438 - val_acc: 0.9311\n",
      "Epoch 79/100\n",
      "0s - loss: 0.2434 - acc: 0.9258 - val_loss: 0.2425 - val_acc: 0.9314\n",
      "Epoch 80/100\n",
      "0s - loss: 0.2427 - acc: 0.9267 - val_loss: 0.2414 - val_acc: 0.9327\n",
      "Epoch 81/100\n",
      "0s - loss: 0.2402 - acc: 0.9272 - val_loss: 0.2411 - val_acc: 0.9321\n",
      "Epoch 82/100\n",
      "0s - loss: 0.2383 - acc: 0.9295 - val_loss: 0.2392 - val_acc: 0.9333\n",
      "Epoch 83/100\n",
      "0s - loss: 0.2416 - acc: 0.9288 - val_loss: 0.2375 - val_acc: 0.9349\n",
      "Epoch 84/100\n",
      "0s - loss: 0.2422 - acc: 0.9283 - val_loss: 0.2371 - val_acc: 0.9337\n",
      "Epoch 85/100\n",
      "0s - loss: 0.2350 - acc: 0.9279 - val_loss: 0.2361 - val_acc: 0.9349\n",
      "Epoch 86/100\n",
      "0s - loss: 0.2321 - acc: 0.9296 - val_loss: 0.2346 - val_acc: 0.9352\n",
      "Epoch 87/100\n",
      "0s - loss: 0.2270 - acc: 0.9325 - val_loss: 0.2329 - val_acc: 0.9346\n",
      "Epoch 88/100\n",
      "0s - loss: 0.2275 - acc: 0.9325 - val_loss: 0.2319 - val_acc: 0.9346\n",
      "Epoch 89/100\n",
      "0s - loss: 0.2234 - acc: 0.9328 - val_loss: 0.2314 - val_acc: 0.9356\n",
      "Epoch 90/100\n",
      "0s - loss: 0.2287 - acc: 0.9317 - val_loss: 0.2302 - val_acc: 0.9349\n",
      "Epoch 91/100\n",
      "0s - loss: 0.2240 - acc: 0.9332 - val_loss: 0.2288 - val_acc: 0.9349\n",
      "Epoch 92/100\n",
      "0s - loss: 0.2181 - acc: 0.9353 - val_loss: 0.2276 - val_acc: 0.9356\n",
      "Epoch 93/100\n",
      "0s - loss: 0.2223 - acc: 0.9328 - val_loss: 0.2263 - val_acc: 0.9362\n",
      "Epoch 94/100\n",
      "0s - loss: 0.2186 - acc: 0.9350 - val_loss: 0.2247 - val_acc: 0.9359\n",
      "Epoch 95/100\n",
      "0s - loss: 0.2126 - acc: 0.9369 - val_loss: 0.2239 - val_acc: 0.9365\n",
      "Epoch 96/100\n",
      "0s - loss: 0.2111 - acc: 0.9367 - val_loss: 0.2236 - val_acc: 0.9368\n",
      "Epoch 97/100\n",
      "0s - loss: 0.2158 - acc: 0.9348 - val_loss: 0.2225 - val_acc: 0.9375\n",
      "Epoch 98/100\n",
      "0s - loss: 0.2126 - acc: 0.9371 - val_loss: 0.2207 - val_acc: 0.9375\n",
      "Epoch 99/100\n",
      "0s - loss: 0.2116 - acc: 0.9375 - val_loss: 0.2201 - val_acc: 0.9375\n",
      "Epoch 100/100\n",
      "0s - loss: 0.2073 - acc: 0.9393 - val_loss: 0.2200 - val_acc: 0.9371\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(800, input_dim=len(X_train[0]), activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "model.add(Dropout(rate=0.25, seed=1))\n",
    "model.add(Dense(70, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "model.add(Dropout(rate=0.25, seed=2))\n",
    "model.add(Dense(10, activation=\"softmax\", kernel_initializer=\"normal\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=128,\n",
    "                    epochs=100, validation_data=(X_test, y_test),\n",
    "                    verbose=2, callbacks=[time_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем не сужать сетку "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9450 samples, validate on 3150 samples\n",
      "Epoch 1/100\n",
      "0s - loss: 2.2135 - acc: 0.2104 - val_loss: 2.0154 - val_acc: 0.5565\n",
      "Epoch 2/100\n",
      "0s - loss: 1.9201 - acc: 0.4392 - val_loss: 1.6342 - val_acc: 0.7365\n",
      "Epoch 3/100\n",
      "0s - loss: 1.5528 - acc: 0.5964 - val_loss: 1.1968 - val_acc: 0.7933\n",
      "Epoch 4/100\n",
      "0s - loss: 1.1949 - acc: 0.6807 - val_loss: 0.8696 - val_acc: 0.8222\n",
      "Epoch 5/100\n",
      "0s - loss: 0.9538 - acc: 0.7316 - val_loss: 0.6891 - val_acc: 0.8425\n",
      "Epoch 6/100\n",
      "0s - loss: 0.8045 - acc: 0.7676 - val_loss: 0.5947 - val_acc: 0.8457\n",
      "Epoch 7/100\n",
      "0s - loss: 0.7242 - acc: 0.7800 - val_loss: 0.5363 - val_acc: 0.8505\n",
      "Epoch 8/100\n",
      "0s - loss: 0.6623 - acc: 0.7980 - val_loss: 0.4909 - val_acc: 0.8632\n",
      "Epoch 9/100\n",
      "0s - loss: 0.6146 - acc: 0.8104 - val_loss: 0.4617 - val_acc: 0.8702\n",
      "Epoch 10/100\n",
      "0s - loss: 0.5814 - acc: 0.8220 - val_loss: 0.4402 - val_acc: 0.8740\n",
      "Epoch 11/100\n",
      "0s - loss: 0.5465 - acc: 0.8366 - val_loss: 0.4194 - val_acc: 0.8803\n",
      "Epoch 12/100\n",
      "0s - loss: 0.5194 - acc: 0.8394 - val_loss: 0.4045 - val_acc: 0.8841\n",
      "Epoch 13/100\n",
      "0s - loss: 0.5008 - acc: 0.8483 - val_loss: 0.3926 - val_acc: 0.8876\n",
      "Epoch 14/100\n",
      "0s - loss: 0.4884 - acc: 0.8516 - val_loss: 0.3789 - val_acc: 0.8911\n",
      "Epoch 15/100\n",
      "0s - loss: 0.4724 - acc: 0.8567 - val_loss: 0.3707 - val_acc: 0.8956\n",
      "Epoch 16/100\n",
      "0s - loss: 0.4616 - acc: 0.8628 - val_loss: 0.3639 - val_acc: 0.8946\n",
      "Epoch 17/100\n",
      "0s - loss: 0.4403 - acc: 0.8678 - val_loss: 0.3532 - val_acc: 0.8987\n",
      "Epoch 18/100\n",
      "0s - loss: 0.4263 - acc: 0.8738 - val_loss: 0.3451 - val_acc: 0.9016\n",
      "Epoch 19/100\n",
      "0s - loss: 0.4196 - acc: 0.8750 - val_loss: 0.3387 - val_acc: 0.9060\n",
      "Epoch 20/100\n",
      "0s - loss: 0.4018 - acc: 0.8801 - val_loss: 0.3350 - val_acc: 0.9044\n",
      "Epoch 21/100\n",
      "0s - loss: 0.3992 - acc: 0.8796 - val_loss: 0.3314 - val_acc: 0.9038\n",
      "Epoch 22/100\n",
      "0s - loss: 0.3938 - acc: 0.8813 - val_loss: 0.3226 - val_acc: 0.9079\n",
      "Epoch 23/100\n",
      "0s - loss: 0.3755 - acc: 0.8854 - val_loss: 0.3197 - val_acc: 0.9092\n",
      "Epoch 24/100\n",
      "0s - loss: 0.3762 - acc: 0.8868 - val_loss: 0.3167 - val_acc: 0.9063\n",
      "Epoch 25/100\n",
      "0s - loss: 0.3634 - acc: 0.8931 - val_loss: 0.3096 - val_acc: 0.9105\n",
      "Epoch 26/100\n",
      "0s - loss: 0.3599 - acc: 0.8967 - val_loss: 0.3045 - val_acc: 0.9114\n",
      "Epoch 27/100\n",
      "0s - loss: 0.3558 - acc: 0.8937 - val_loss: 0.3036 - val_acc: 0.9127\n",
      "Epoch 28/100\n",
      "0s - loss: 0.3457 - acc: 0.8968 - val_loss: 0.2991 - val_acc: 0.9127\n",
      "Epoch 29/100\n",
      "0s - loss: 0.3319 - acc: 0.8988 - val_loss: 0.2954 - val_acc: 0.9146\n",
      "Epoch 30/100\n",
      "0s - loss: 0.3365 - acc: 0.8988 - val_loss: 0.2896 - val_acc: 0.9168\n",
      "Epoch 31/100\n",
      "0s - loss: 0.3287 - acc: 0.9038 - val_loss: 0.2879 - val_acc: 0.9159\n",
      "Epoch 32/100\n",
      "0s - loss: 0.3193 - acc: 0.9085 - val_loss: 0.2851 - val_acc: 0.9181\n",
      "Epoch 33/100\n",
      "0s - loss: 0.3172 - acc: 0.9048 - val_loss: 0.2819 - val_acc: 0.9187\n",
      "Epoch 34/100\n",
      "0s - loss: 0.3036 - acc: 0.9091 - val_loss: 0.2782 - val_acc: 0.9190\n",
      "Epoch 35/100\n",
      "0s - loss: 0.3076 - acc: 0.9079 - val_loss: 0.2764 - val_acc: 0.9175\n",
      "Epoch 36/100\n",
      "0s - loss: 0.3035 - acc: 0.9063 - val_loss: 0.2741 - val_acc: 0.9184\n",
      "Epoch 37/100\n",
      "0s - loss: 0.2941 - acc: 0.9126 - val_loss: 0.2706 - val_acc: 0.9210\n",
      "Epoch 38/100\n",
      "0s - loss: 0.2952 - acc: 0.9108 - val_loss: 0.2652 - val_acc: 0.9229\n",
      "Epoch 39/100\n",
      "0s - loss: 0.2916 - acc: 0.9128 - val_loss: 0.2635 - val_acc: 0.9229\n",
      "Epoch 40/100\n",
      "0s - loss: 0.2844 - acc: 0.9152 - val_loss: 0.2631 - val_acc: 0.9219\n",
      "Epoch 41/100\n",
      "0s - loss: 0.2823 - acc: 0.9153 - val_loss: 0.2603 - val_acc: 0.9241\n",
      "Epoch 42/100\n",
      "0s - loss: 0.2787 - acc: 0.9176 - val_loss: 0.2580 - val_acc: 0.9254\n",
      "Epoch 43/100\n",
      "0s - loss: 0.2743 - acc: 0.9163 - val_loss: 0.2561 - val_acc: 0.9260\n",
      "Epoch 44/100\n",
      "0s - loss: 0.2670 - acc: 0.9182 - val_loss: 0.2535 - val_acc: 0.9260\n",
      "Epoch 45/100\n",
      "0s - loss: 0.2656 - acc: 0.9216 - val_loss: 0.2527 - val_acc: 0.9260\n",
      "Epoch 46/100\n",
      "0s - loss: 0.2573 - acc: 0.9233 - val_loss: 0.2495 - val_acc: 0.9283\n",
      "Epoch 47/100\n",
      "0s - loss: 0.2617 - acc: 0.9223 - val_loss: 0.2461 - val_acc: 0.9308\n",
      "Epoch 48/100\n",
      "0s - loss: 0.2544 - acc: 0.9251 - val_loss: 0.2449 - val_acc: 0.9295\n",
      "Epoch 49/100\n",
      "0s - loss: 0.2482 - acc: 0.9248 - val_loss: 0.2431 - val_acc: 0.9305\n",
      "Epoch 50/100\n",
      "0s - loss: 0.2478 - acc: 0.9266 - val_loss: 0.2419 - val_acc: 0.9308\n",
      "Epoch 51/100\n",
      "0s - loss: 0.2386 - acc: 0.9268 - val_loss: 0.2394 - val_acc: 0.9337\n",
      "Epoch 52/100\n",
      "0s - loss: 0.2433 - acc: 0.9248 - val_loss: 0.2376 - val_acc: 0.9317\n",
      "Epoch 53/100\n",
      "0s - loss: 0.2340 - acc: 0.9296 - val_loss: 0.2363 - val_acc: 0.9333\n",
      "Epoch 54/100\n",
      "0s - loss: 0.2363 - acc: 0.9276 - val_loss: 0.2338 - val_acc: 0.9346\n",
      "Epoch 55/100\n",
      "0s - loss: 0.2371 - acc: 0.9290 - val_loss: 0.2346 - val_acc: 0.9330\n",
      "Epoch 56/100\n",
      "0s - loss: 0.2324 - acc: 0.9317 - val_loss: 0.2303 - val_acc: 0.9337\n",
      "Epoch 57/100\n",
      "0s - loss: 0.2275 - acc: 0.9312 - val_loss: 0.2300 - val_acc: 0.9359\n",
      "Epoch 58/100\n",
      "0s - loss: 0.2259 - acc: 0.9320 - val_loss: 0.2291 - val_acc: 0.9337\n",
      "Epoch 59/100\n",
      "0s - loss: 0.2204 - acc: 0.9345 - val_loss: 0.2275 - val_acc: 0.9365\n",
      "Epoch 60/100\n",
      "0s - loss: 0.2137 - acc: 0.9361 - val_loss: 0.2258 - val_acc: 0.9375\n",
      "Epoch 61/100\n",
      "0s - loss: 0.2114 - acc: 0.9371 - val_loss: 0.2233 - val_acc: 0.9371\n",
      "Epoch 62/100\n",
      "0s - loss: 0.2084 - acc: 0.9383 - val_loss: 0.2221 - val_acc: 0.9375\n",
      "Epoch 63/100\n",
      "0s - loss: 0.2067 - acc: 0.9372 - val_loss: 0.2202 - val_acc: 0.9365\n",
      "Epoch 64/100\n",
      "0s - loss: 0.2092 - acc: 0.9374 - val_loss: 0.2189 - val_acc: 0.9378\n",
      "Epoch 65/100\n",
      "0s - loss: 0.2053 - acc: 0.9388 - val_loss: 0.2194 - val_acc: 0.9394\n",
      "Epoch 66/100\n",
      "0s - loss: 0.2013 - acc: 0.9381 - val_loss: 0.2170 - val_acc: 0.9365\n",
      "Epoch 67/100\n",
      "0s - loss: 0.2008 - acc: 0.9366 - val_loss: 0.2168 - val_acc: 0.9394\n",
      "Epoch 68/100\n",
      "0s - loss: 0.1920 - acc: 0.9440 - val_loss: 0.2154 - val_acc: 0.9390\n",
      "Epoch 69/100\n",
      "0s - loss: 0.1943 - acc: 0.9407 - val_loss: 0.2132 - val_acc: 0.9413\n",
      "Epoch 70/100\n",
      "0s - loss: 0.1899 - acc: 0.9420 - val_loss: 0.2117 - val_acc: 0.9397\n",
      "Epoch 71/100\n",
      "0s - loss: 0.1924 - acc: 0.9386 - val_loss: 0.2118 - val_acc: 0.9394\n",
      "Epoch 72/100\n",
      "0s - loss: 0.1878 - acc: 0.9443 - val_loss: 0.2113 - val_acc: 0.9400\n",
      "Epoch 73/100\n",
      "0s - loss: 0.1852 - acc: 0.9439 - val_loss: 0.2094 - val_acc: 0.9406\n",
      "Epoch 74/100\n",
      "0s - loss: 0.1809 - acc: 0.9439 - val_loss: 0.2088 - val_acc: 0.9403\n",
      "Epoch 75/100\n",
      "0s - loss: 0.1819 - acc: 0.9454 - val_loss: 0.2077 - val_acc: 0.9410\n",
      "Epoch 76/100\n",
      "0s - loss: 0.1836 - acc: 0.9439 - val_loss: 0.2064 - val_acc: 0.9410\n",
      "Epoch 77/100\n",
      "0s - loss: 0.1801 - acc: 0.9461 - val_loss: 0.2054 - val_acc: 0.9410\n",
      "Epoch 78/100\n",
      "0s - loss: 0.1757 - acc: 0.9471 - val_loss: 0.2043 - val_acc: 0.9419\n",
      "Epoch 79/100\n",
      "0s - loss: 0.1715 - acc: 0.9484 - val_loss: 0.2024 - val_acc: 0.9416\n",
      "Epoch 80/100\n",
      "0s - loss: 0.1758 - acc: 0.9442 - val_loss: 0.2036 - val_acc: 0.9419\n",
      "Epoch 81/100\n",
      "0s - loss: 0.1705 - acc: 0.9484 - val_loss: 0.2026 - val_acc: 0.9416\n",
      "Epoch 82/100\n",
      "0s - loss: 0.1686 - acc: 0.9494 - val_loss: 0.2013 - val_acc: 0.9425\n",
      "Epoch 83/100\n",
      "0s - loss: 0.1632 - acc: 0.9484 - val_loss: 0.2009 - val_acc: 0.9429\n",
      "Epoch 84/100\n",
      "0s - loss: 0.1701 - acc: 0.9470 - val_loss: 0.1980 - val_acc: 0.9429\n",
      "Epoch 85/100\n",
      "0s - loss: 0.1662 - acc: 0.9495 - val_loss: 0.1979 - val_acc: 0.9435\n",
      "Epoch 86/100\n",
      "0s - loss: 0.1586 - acc: 0.9496 - val_loss: 0.1978 - val_acc: 0.9441\n",
      "Epoch 87/100\n",
      "0s - loss: 0.1555 - acc: 0.9512 - val_loss: 0.1975 - val_acc: 0.9438\n",
      "Epoch 88/100\n",
      "0s - loss: 0.1599 - acc: 0.9513 - val_loss: 0.1983 - val_acc: 0.9438\n",
      "Epoch 89/100\n",
      "0s - loss: 0.1604 - acc: 0.9502 - val_loss: 0.1950 - val_acc: 0.9441\n",
      "Epoch 90/100\n",
      "0s - loss: 0.1530 - acc: 0.9533 - val_loss: 0.1936 - val_acc: 0.9444\n",
      "Epoch 91/100\n",
      "0s - loss: 0.1527 - acc: 0.9542 - val_loss: 0.1948 - val_acc: 0.9460\n",
      "Epoch 92/100\n",
      "0s - loss: 0.1512 - acc: 0.9540 - val_loss: 0.1915 - val_acc: 0.9454\n",
      "Epoch 93/100\n",
      "0s - loss: 0.1476 - acc: 0.9535 - val_loss: 0.1916 - val_acc: 0.9451\n",
      "Epoch 94/100\n",
      "0s - loss: 0.1464 - acc: 0.9540 - val_loss: 0.1910 - val_acc: 0.9444\n",
      "Epoch 95/100\n",
      "0s - loss: 0.1424 - acc: 0.9577 - val_loss: 0.1913 - val_acc: 0.9470\n",
      "Epoch 96/100\n",
      "0s - loss: 0.1476 - acc: 0.9546 - val_loss: 0.1910 - val_acc: 0.9441\n",
      "Epoch 97/100\n",
      "0s - loss: 0.1438 - acc: 0.9558 - val_loss: 0.1912 - val_acc: 0.9448\n",
      "Epoch 98/100\n",
      "0s - loss: 0.1386 - acc: 0.9585 - val_loss: 0.1903 - val_acc: 0.9473\n",
      "Epoch 99/100\n",
      "0s - loss: 0.1391 - acc: 0.9586 - val_loss: 0.1908 - val_acc: 0.9454\n",
      "Epoch 100/100\n",
      "0s - loss: 0.1411 - acc: 0.9556 - val_loss: 0.1876 - val_acc: 0.9467\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(500, input_dim=len(X_train[0]), activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "model.add(Dropout(rate=0.25, seed=1))\n",
    "model.add(Dense(500, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "model.add(Dropout(rate=0.25, seed=2))\n",
    "model.add(Dense(500, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "model.add(Dropout(rate=0.25, seed=2))\n",
    "model.add(Dense(10, activation=\"softmax\", kernel_initializer=\"normal\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=128,\n",
    "                    epochs=100, validation_data=(X_test, y_test),\n",
    "                    verbose=2, callbacks=[time_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Больше слоев!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9450 samples, validate on 3150 samples\n",
      "Epoch 1/100\n",
      "0s - loss: 2.3193 - acc: 0.1304 - val_loss: 2.1431 - val_acc: 0.3984\n",
      "Epoch 2/100\n",
      "0s - loss: 2.1774 - acc: 0.2180 - val_loss: 1.9053 - val_acc: 0.5714\n",
      "Epoch 3/100\n",
      "0s - loss: 1.9418 - acc: 0.3471 - val_loss: 1.4520 - val_acc: 0.6311\n",
      "Epoch 4/100\n",
      "0s - loss: 1.5557 - acc: 0.4728 - val_loss: 0.9966 - val_acc: 0.7305\n",
      "Epoch 5/100\n",
      "0s - loss: 1.2435 - acc: 0.5671 - val_loss: 0.7808 - val_acc: 0.7641\n",
      "Epoch 6/100\n",
      "0s - loss: 1.0531 - acc: 0.6241 - val_loss: 0.6633 - val_acc: 0.8025\n",
      "Epoch 7/100\n",
      "0s - loss: 0.9199 - acc: 0.6803 - val_loss: 0.5884 - val_acc: 0.8143\n",
      "Epoch 8/100\n",
      "0s - loss: 0.8253 - acc: 0.7175 - val_loss: 0.5257 - val_acc: 0.8352\n",
      "Epoch 9/100\n",
      "0s - loss: 0.7569 - acc: 0.7451 - val_loss: 0.4888 - val_acc: 0.8486\n",
      "Epoch 10/100\n",
      "0s - loss: 0.6916 - acc: 0.7709 - val_loss: 0.4527 - val_acc: 0.8616\n",
      "Epoch 11/100\n",
      "0s - loss: 0.6275 - acc: 0.7939 - val_loss: 0.4226 - val_acc: 0.8711\n",
      "Epoch 12/100\n",
      "0s - loss: 0.6106 - acc: 0.8008 - val_loss: 0.4144 - val_acc: 0.8711\n",
      "Epoch 13/100\n",
      "0s - loss: 0.5715 - acc: 0.8166 - val_loss: 0.3846 - val_acc: 0.8806\n",
      "Epoch 14/100\n",
      "0s - loss: 0.5454 - acc: 0.8242 - val_loss: 0.3757 - val_acc: 0.8829\n",
      "Epoch 15/100\n",
      "0s - loss: 0.5254 - acc: 0.8290 - val_loss: 0.3597 - val_acc: 0.8908\n",
      "Epoch 16/100\n",
      "0s - loss: 0.5012 - acc: 0.8423 - val_loss: 0.3468 - val_acc: 0.8946\n",
      "Epoch 17/100\n",
      "0s - loss: 0.4815 - acc: 0.8479 - val_loss: 0.3352 - val_acc: 0.8981\n",
      "Epoch 18/100\n",
      "0s - loss: 0.4721 - acc: 0.8547 - val_loss: 0.3283 - val_acc: 0.8946\n",
      "Epoch 19/100\n",
      "0s - loss: 0.4536 - acc: 0.8562 - val_loss: 0.3199 - val_acc: 0.9035\n",
      "Epoch 20/100\n",
      "0s - loss: 0.4382 - acc: 0.8632 - val_loss: 0.3110 - val_acc: 0.9063\n",
      "Epoch 21/100\n",
      "0s - loss: 0.4271 - acc: 0.8649 - val_loss: 0.3023 - val_acc: 0.9105\n",
      "Epoch 22/100\n",
      "0s - loss: 0.4102 - acc: 0.8726 - val_loss: 0.2966 - val_acc: 0.9114\n",
      "Epoch 23/100\n",
      "0s - loss: 0.3879 - acc: 0.8759 - val_loss: 0.2927 - val_acc: 0.9114\n",
      "Epoch 24/100\n",
      "0s - loss: 0.3866 - acc: 0.8824 - val_loss: 0.2832 - val_acc: 0.9162\n",
      "Epoch 25/100\n",
      "0s - loss: 0.3742 - acc: 0.8831 - val_loss: 0.2815 - val_acc: 0.9168\n",
      "Epoch 26/100\n",
      "0s - loss: 0.3655 - acc: 0.8883 - val_loss: 0.2743 - val_acc: 0.9187\n",
      "Epoch 27/100\n",
      "0s - loss: 0.3571 - acc: 0.8844 - val_loss: 0.2699 - val_acc: 0.9203\n",
      "Epoch 28/100\n",
      "0s - loss: 0.3443 - acc: 0.8924 - val_loss: 0.2645 - val_acc: 0.9206\n",
      "Epoch 29/100\n",
      "0s - loss: 0.3382 - acc: 0.8960 - val_loss: 0.2599 - val_acc: 0.9222\n",
      "Epoch 30/100\n",
      "0s - loss: 0.3256 - acc: 0.9004 - val_loss: 0.2572 - val_acc: 0.9241\n",
      "Epoch 31/100\n",
      "0s - loss: 0.3302 - acc: 0.8967 - val_loss: 0.2559 - val_acc: 0.9229\n",
      "Epoch 32/100\n",
      "0s - loss: 0.3174 - acc: 0.9030 - val_loss: 0.2500 - val_acc: 0.9260\n",
      "Epoch 33/100\n",
      "0s - loss: 0.3040 - acc: 0.9077 - val_loss: 0.2477 - val_acc: 0.9276\n",
      "Epoch 34/100\n",
      "0s - loss: 0.2996 - acc: 0.9079 - val_loss: 0.2402 - val_acc: 0.9270\n",
      "Epoch 35/100\n",
      "0s - loss: 0.2913 - acc: 0.9098 - val_loss: 0.2383 - val_acc: 0.9292\n",
      "Epoch 36/100\n",
      "0s - loss: 0.2812 - acc: 0.9116 - val_loss: 0.2356 - val_acc: 0.9298\n",
      "Epoch 37/100\n",
      "0s - loss: 0.2808 - acc: 0.9122 - val_loss: 0.2329 - val_acc: 0.9308\n",
      "Epoch 38/100\n",
      "0s - loss: 0.2787 - acc: 0.9142 - val_loss: 0.2294 - val_acc: 0.9321\n",
      "Epoch 39/100\n",
      "0s - loss: 0.2663 - acc: 0.9172 - val_loss: 0.2309 - val_acc: 0.9305\n",
      "Epoch 40/100\n",
      "0s - loss: 0.2614 - acc: 0.9167 - val_loss: 0.2269 - val_acc: 0.9321\n",
      "Epoch 41/100\n",
      "0s - loss: 0.2572 - acc: 0.9196 - val_loss: 0.2225 - val_acc: 0.9343\n",
      "Epoch 42/100\n",
      "0s - loss: 0.2500 - acc: 0.9210 - val_loss: 0.2241 - val_acc: 0.9330\n",
      "Epoch 43/100\n",
      "0s - loss: 0.2477 - acc: 0.9223 - val_loss: 0.2217 - val_acc: 0.9371\n",
      "Epoch 44/100\n",
      "0s - loss: 0.2352 - acc: 0.9279 - val_loss: 0.2180 - val_acc: 0.9384\n",
      "Epoch 45/100\n",
      "0s - loss: 0.2346 - acc: 0.9261 - val_loss: 0.2154 - val_acc: 0.9371\n",
      "Epoch 46/100\n",
      "0s - loss: 0.2292 - acc: 0.9302 - val_loss: 0.2132 - val_acc: 0.9378\n",
      "Epoch 47/100\n",
      "0s - loss: 0.2247 - acc: 0.9331 - val_loss: 0.2114 - val_acc: 0.9387\n",
      "Epoch 48/100\n",
      "0s - loss: 0.2272 - acc: 0.9272 - val_loss: 0.2094 - val_acc: 0.9406\n",
      "Epoch 49/100\n",
      "0s - loss: 0.2204 - acc: 0.9305 - val_loss: 0.2123 - val_acc: 0.9390\n",
      "Epoch 50/100\n",
      "0s - loss: 0.2146 - acc: 0.9324 - val_loss: 0.2076 - val_acc: 0.9400\n",
      "Epoch 51/100\n",
      "0s - loss: 0.2154 - acc: 0.9331 - val_loss: 0.2031 - val_acc: 0.9429\n",
      "Epoch 52/100\n",
      "0s - loss: 0.2040 - acc: 0.9361 - val_loss: 0.2036 - val_acc: 0.9429\n",
      "Epoch 53/100\n",
      "0s - loss: 0.2019 - acc: 0.9400 - val_loss: 0.2044 - val_acc: 0.9422\n",
      "Epoch 54/100\n",
      "0s - loss: 0.1977 - acc: 0.9398 - val_loss: 0.2013 - val_acc: 0.9444\n",
      "Epoch 55/100\n",
      "0s - loss: 0.1909 - acc: 0.9425 - val_loss: 0.2032 - val_acc: 0.9451\n",
      "Epoch 56/100\n",
      "0s - loss: 0.1907 - acc: 0.9422 - val_loss: 0.2004 - val_acc: 0.9451\n",
      "Epoch 57/100\n",
      "0s - loss: 0.1836 - acc: 0.9436 - val_loss: 0.1978 - val_acc: 0.9460\n",
      "Epoch 58/100\n",
      "0s - loss: 0.1894 - acc: 0.9417 - val_loss: 0.1989 - val_acc: 0.9444\n",
      "Epoch 59/100\n",
      "0s - loss: 0.1912 - acc: 0.9381 - val_loss: 0.1957 - val_acc: 0.9457\n",
      "Epoch 60/100\n",
      "0s - loss: 0.1753 - acc: 0.9443 - val_loss: 0.1943 - val_acc: 0.9476\n",
      "Epoch 61/100\n",
      "0s - loss: 0.1803 - acc: 0.9416 - val_loss: 0.1948 - val_acc: 0.9457\n",
      "Epoch 62/100\n",
      "0s - loss: 0.1769 - acc: 0.9432 - val_loss: 0.1962 - val_acc: 0.9463\n",
      "Epoch 63/100\n",
      "0s - loss: 0.1665 - acc: 0.9468 - val_loss: 0.1950 - val_acc: 0.9463\n",
      "Epoch 64/100\n",
      "0s - loss: 0.1641 - acc: 0.9471 - val_loss: 0.1929 - val_acc: 0.9479\n",
      "Epoch 65/100\n",
      "0s - loss: 0.1585 - acc: 0.9486 - val_loss: 0.1933 - val_acc: 0.9476\n",
      "Epoch 66/100\n",
      "0s - loss: 0.1616 - acc: 0.9472 - val_loss: 0.1919 - val_acc: 0.9479\n",
      "Epoch 67/100\n",
      "0s - loss: 0.1569 - acc: 0.9471 - val_loss: 0.1898 - val_acc: 0.9476\n",
      "Epoch 68/100\n",
      "0s - loss: 0.1576 - acc: 0.9498 - val_loss: 0.1893 - val_acc: 0.9483\n",
      "Epoch 69/100\n",
      "0s - loss: 0.1522 - acc: 0.9511 - val_loss: 0.1892 - val_acc: 0.9498\n",
      "Epoch 70/100\n",
      "0s - loss: 0.1524 - acc: 0.9497 - val_loss: 0.1868 - val_acc: 0.9479\n",
      "Epoch 71/100\n",
      "0s - loss: 0.1477 - acc: 0.9538 - val_loss: 0.1914 - val_acc: 0.9486\n",
      "Epoch 72/100\n",
      "0s - loss: 0.1470 - acc: 0.9533 - val_loss: 0.1863 - val_acc: 0.9489\n",
      "Epoch 73/100\n",
      "0s - loss: 0.1394 - acc: 0.9539 - val_loss: 0.1867 - val_acc: 0.9492\n",
      "Epoch 74/100\n",
      "0s - loss: 0.1390 - acc: 0.9581 - val_loss: 0.1899 - val_acc: 0.9486\n",
      "Epoch 75/100\n",
      "0s - loss: 0.1395 - acc: 0.9559 - val_loss: 0.1890 - val_acc: 0.9483\n",
      "Epoch 76/100\n",
      "0s - loss: 0.1343 - acc: 0.9595 - val_loss: 0.1903 - val_acc: 0.9495\n",
      "Epoch 77/100\n",
      "0s - loss: 0.1354 - acc: 0.9534 - val_loss: 0.1861 - val_acc: 0.9502\n",
      "Epoch 78/100\n",
      "0s - loss: 0.1380 - acc: 0.9554 - val_loss: 0.1848 - val_acc: 0.9502\n",
      "Epoch 79/100\n",
      "0s - loss: 0.1260 - acc: 0.9606 - val_loss: 0.1869 - val_acc: 0.9505\n",
      "Epoch 80/100\n",
      "0s - loss: 0.1256 - acc: 0.9600 - val_loss: 0.1858 - val_acc: 0.9502\n",
      "Epoch 81/100\n",
      "0s - loss: 0.1222 - acc: 0.9625 - val_loss: 0.1863 - val_acc: 0.9514\n",
      "Epoch 82/100\n",
      "0s - loss: 0.1227 - acc: 0.9608 - val_loss: 0.1870 - val_acc: 0.9502\n",
      "Epoch 83/100\n",
      "0s - loss: 0.1212 - acc: 0.9608 - val_loss: 0.1835 - val_acc: 0.9505\n",
      "Epoch 84/100\n",
      "0s - loss: 0.1189 - acc: 0.9623 - val_loss: 0.1876 - val_acc: 0.9495\n",
      "Epoch 85/100\n",
      "0s - loss: 0.1158 - acc: 0.9631 - val_loss: 0.1849 - val_acc: 0.9505\n",
      "Epoch 86/100\n",
      "0s - loss: 0.1124 - acc: 0.9641 - val_loss: 0.1863 - val_acc: 0.9492\n",
      "Epoch 87/100\n",
      "0s - loss: 0.1129 - acc: 0.9620 - val_loss: 0.1887 - val_acc: 0.9530\n",
      "Epoch 88/100\n",
      "0s - loss: 0.1085 - acc: 0.9661 - val_loss: 0.1868 - val_acc: 0.9517\n",
      "Epoch 89/100\n",
      "0s - loss: 0.1103 - acc: 0.9641 - val_loss: 0.1830 - val_acc: 0.9502\n",
      "Epoch 90/100\n",
      "0s - loss: 0.1090 - acc: 0.9649 - val_loss: 0.1863 - val_acc: 0.9517\n",
      "Epoch 91/100\n",
      "0s - loss: 0.1052 - acc: 0.9657 - val_loss: 0.1866 - val_acc: 0.9498\n",
      "Epoch 92/100\n",
      "0s - loss: 0.1043 - acc: 0.9663 - val_loss: 0.1832 - val_acc: 0.9511\n",
      "Epoch 93/100\n",
      "0s - loss: 0.1049 - acc: 0.9661 - val_loss: 0.1861 - val_acc: 0.9514\n",
      "Epoch 94/100\n",
      "0s - loss: 0.1022 - acc: 0.9666 - val_loss: 0.1863 - val_acc: 0.9505\n",
      "Epoch 95/100\n",
      "0s - loss: 0.1044 - acc: 0.9649 - val_loss: 0.1855 - val_acc: 0.9514\n",
      "Epoch 96/100\n",
      "0s - loss: 0.0981 - acc: 0.9694 - val_loss: 0.1861 - val_acc: 0.9505\n",
      "Epoch 97/100\n",
      "0s - loss: 0.0983 - acc: 0.9690 - val_loss: 0.1830 - val_acc: 0.9527\n",
      "Epoch 98/100\n",
      "0s - loss: 0.0976 - acc: 0.9699 - val_loss: 0.1864 - val_acc: 0.9527\n",
      "Epoch 99/100\n",
      "0s - loss: 0.0916 - acc: 0.9705 - val_loss: 0.1845 - val_acc: 0.9521\n",
      "Epoch 100/100\n",
      "0s - loss: 0.0960 - acc: 0.9697 - val_loss: 0.1870 - val_acc: 0.9505\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(800, input_dim=len(X_train[0]), activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "model.add(Dropout(rate=0.25, seed=1))\n",
    "model.add(Dense(800, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "model.add(Dropout(rate=0.25, seed=2))\n",
    "model.add(Dense(800, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "model.add(Dropout(rate=0.25, seed=2))\n",
    "model.add(Dense(500, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "model.add(Dropout(rate=0.25, seed=2))\n",
    "model.add(Dense(500, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "model.add(Dropout(rate=0.25, seed=2))\n",
    "model.add(Dense(500, activation=\"relu\", kernel_initializer=\"normal\"))\n",
    "model.add(Dropout(rate=0.25, seed=2))\n",
    "model.add(Dense(10, activation=\"softmax\", kernel_initializer=\"normal\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=128,\n",
    "                    epochs=100, validation_data=(X_test, y_test),\n",
    "                    verbose=2, callbacks=[time_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберите 1-2 модели, которые дали наилучший результат. Подберите параметры в них: попробуйте выбрать различные функции активации, коэффициент Dropout, коэффициенты регуляризации. Проведите порядка 10 экспериментов. Как настройка слоев влияет на качество модели? Сильно ли различается результат?\n",
    "\n",
    "Коэффициенты регуляризации передаются в Dense слои с помощью `kernel_regularizer=l2(l2_coef)`. Рекомендуется провести эксперимент с коэффициентами от $10^{-6}$ до $10^{-4}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попытайтесь подобрать для SGD начальный `learning_rate`, темп его снижения `decay`. Диапазоны параметров, с которыми рекомендуется провести эксперименты:\n",
    "- `learning rate`: от 0.005 до 0.1,\n",
    "- `decay`: от $10^{-9}$ до $10^{-5}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем последнюю модель, будем тюнить ее. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd394bf0228462e89a82faa5313ab71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2976/3150 [===========================>..] - ETA: 0s\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "regs = [0.000001, 0.00005, 0.00001, 0.0005, 0.0001]\n",
    "\n",
    "for l in tqdm(regs):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(800, input_dim=len(X_train[0]), activation=\"relu\", \n",
    "                    kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=1))\n",
    "    model.add(Dense(800, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=2))\n",
    "    model.add(Dense(800, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=2))\n",
    "    model.add(Dense(500, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=2))\n",
    "    model.add(Dense(500, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=2))\n",
    "    model.add(Dense(500, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=2))\n",
    "    model.add(Dense(10, activation=\"softmax\", kernel_initializer=\"normal\"))\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit(X_train, y_train, batch_size=128,\n",
    "                        epochs=100, validation_data=(X_test, y_test),\n",
    "                        verbose=0, callbacks=[time_callback])\n",
    "    scores.append(model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0.954603174603\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(scores))\n",
    "print(np.max(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший коэф. регуляризации получился 0.0001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc896c074790446cafc6a9dc9965db88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2720/3150 [========================>.....] - ETA: 0s\n",
      "7\n",
      "0.959682539683\n"
     ]
    }
   ],
   "source": [
    "l = 0.0001\n",
    "\n",
    "lrates = [0.005, 0.01, 0.05, 0.1]\n",
    "\n",
    "for lr in tqdm(lrates):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(800, input_dim=len(X_train[0]), activation=\"relu\", \n",
    "                    kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=1))\n",
    "    model.add(Dense(800, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=2))\n",
    "    model.add(Dense(800, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=2))\n",
    "    model.add(Dense(500, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=2))\n",
    "    model.add(Dense(500, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=2))\n",
    "    model.add(Dense(500, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=2))\n",
    "    model.add(Dense(10, activation=\"softmax\", kernel_initializer=\"normal\"))\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=SGD(lr=lr), metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit(X_train, y_train, batch_size=128,\n",
    "                        epochs=100, validation_data=(X_test, y_test),\n",
    "                        verbose=0, callbacks=[time_callback])\n",
    "    scores.append(model.evaluate(X_test, y_test)[1])\n",
    "print(np.argmax(scores))\n",
    "print(np.max(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшая скорость обучения вышла 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235fa297a6aa4a999c57bff70f680ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3150/3150 [==============================] - 0s     \n",
      "3072/3150 [============================>.] - ETA: 0s\n",
      "12\n",
      "0.962222222222\n"
     ]
    }
   ],
   "source": [
    "decays = [0.000000001, 0.00000001, 0.0000001, 0.000001, 0.00001]\n",
    "for dec in tqdm(decays):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(800, input_dim=len(X_train[0]), activation=\"relu\", \n",
    "                    kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=1))\n",
    "    model.add(Dense(800, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=2))\n",
    "    model.add(Dense(800, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=2))\n",
    "    model.add(Dense(500, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=2))\n",
    "    model.add(Dense(500, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=2))\n",
    "    model.add(Dense(500, activation=\"relu\", kernel_initializer=\"normal\", kernel_regularizer=l2(l)))\n",
    "    model.add(Dropout(rate=0.25, seed=2))\n",
    "    model.add(Dense(10, activation=\"softmax\", kernel_initializer=\"normal\"))\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=SGD(lr=lr), metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit(X_train, y_train, batch_size=128,\n",
    "                        epochs=100, validation_data=(X_test, y_test),\n",
    "                        verbose=0, callbacks=[time_callback])\n",
    "    scores.append(model.evaluate(X_test, y_test)[1])\n",
    "print(np.argmax(scores))\n",
    "print(np.max(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший decay оказался равен 0.000001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберите лучшую сеть, обучите ее и оцените качество. Сделайте графики зависимости качества классификации от этапа настройки параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzwAAAHiCAYAAAAtXcmYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPAAAwLD+P4vN/7K22Bh0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fef7c10dcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = ['1.baseline',  '2.шаманизм', '3.l2', '4.lr', '5.decay']\n",
    "\n",
    "y_acc = [0.8806, 0.9505, 0.954603174603, 0.959682539683, 0.962222222222] \n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.scatter(x, y_acc)\n",
    "plt.title('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый большой скачок качества дал шаманизм со слоями и числом нейронов в них. Следующие шаги тоже дали ощутимый прирост, но не такой большой. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сверточная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно, правильно было бы попробовать и ее, но тогда задание получится чрезмерно трудоемким. =) Поэтому этот пункт необязателен к выполнению. Однако, вы можете самостоятельно разобраться в конструировании сверточных сетей и оценить качество их работы. Для построения сверточного блока вам необходимо реализовать последовательность свертки ([Convolution2D](https://keras.io/layers/convolutional/#conv2d)), нелинейности (ReLU) и пулинга ([MaxPooling2D](https://keras.io/layers/pooling/#maxpooling2d)). Почитайте документацию, посмотрите примеры сверточных сетей в аналогичных работах.\n",
    "\n",
    "Да, бонусные баллы (до +6) за реализацию этого пункта ставиться будут."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cделаем что-то вроде очень небольшой VGG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop #более эффективен, чем SDG \n",
    "from keras.preprocessing.image import ImageDataGenerator #будем применять аугментацию в реальном времени \n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten \n",
    "from keras.callbacks import ReduceLROnPlateau #уменьшение learning rate на плато может помочь \n",
    "\n",
    "X_train = X_train.reshape(-1,28,28,1)\n",
    "X_test = X_test.reshape(-1,28,28,1)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation=\"relu\", kernel_initializer='normal'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(200, activation=\"relu\", kernel_initializer='normal'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                 patience=3, \n",
    "                                 factor=0.5, \n",
    "                                 min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30 \n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = ImageDataGenerator(rotation_range=10,  #случайный поворот от 0 до 10 градусов\n",
    "                             zoom_range = 0.1, #случайный зум\n",
    "                             width_shift_range=0.1,  #случайный сдвиг по ширине\n",
    "                             height_shift_range=0.1,  #случайный сдвиг по высоте \n",
    "                            ) \n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3s - loss: 1.2690 - acc: 0.5647 - val_loss: 0.1923 - val_acc: 0.9416\n",
      "Epoch 2/30\n",
      "2s - loss: 0.3984 - acc: 0.8745 - val_loss: 0.1089 - val_acc: 0.9660\n",
      "Epoch 3/30\n",
      "2s - loss: 0.2676 - acc: 0.9166 - val_loss: 0.0851 - val_acc: 0.9762\n",
      "Epoch 4/30\n",
      "2s - loss: 0.1861 - acc: 0.9431 - val_loss: 0.0996 - val_acc: 0.9686\n",
      "Epoch 5/30\n",
      "2s - loss: 0.1655 - acc: 0.9496 - val_loss: 0.0780 - val_acc: 0.9778\n",
      "Epoch 6/30\n",
      "2s - loss: 0.1329 - acc: 0.9591 - val_loss: 0.0726 - val_acc: 0.9765\n",
      "Epoch 7/30\n",
      "2s - loss: 0.1301 - acc: 0.9602 - val_loss: 0.0510 - val_acc: 0.9844\n",
      "Epoch 8/30\n",
      "2s - loss: 0.1117 - acc: 0.9662 - val_loss: 0.0489 - val_acc: 0.9848\n",
      "Epoch 9/30\n",
      "2s - loss: 0.1031 - acc: 0.9691 - val_loss: 0.0454 - val_acc: 0.9860\n",
      "Epoch 10/30\n",
      "2s - loss: 0.0942 - acc: 0.9723 - val_loss: 0.0477 - val_acc: 0.9844\n",
      "Epoch 11/30\n",
      "2s - loss: 0.0898 - acc: 0.9718 - val_loss: 0.0469 - val_acc: 0.9829\n",
      "Epoch 12/30\n",
      "2s - loss: 0.0888 - acc: 0.9726 - val_loss: 0.0531 - val_acc: 0.9819\n",
      "Epoch 13/30\n",
      "2s - loss: 0.0770 - acc: 0.9764 - val_loss: 0.0386 - val_acc: 0.9873\n",
      "Epoch 14/30\n",
      "2s - loss: 0.0798 - acc: 0.9759 - val_loss: 0.0513 - val_acc: 0.9848\n",
      "Epoch 15/30\n",
      "2s - loss: 0.0765 - acc: 0.9767 - val_loss: 0.0349 - val_acc: 0.9902\n",
      "Epoch 16/30\n",
      "2s - loss: 0.0718 - acc: 0.9774 - val_loss: 0.0554 - val_acc: 0.9860\n",
      "Epoch 17/30\n",
      "2s - loss: 0.0713 - acc: 0.9799 - val_loss: 0.0311 - val_acc: 0.9914\n",
      "Epoch 18/30\n",
      "2s - loss: 0.0617 - acc: 0.9802 - val_loss: 0.0469 - val_acc: 0.9876\n",
      "Epoch 19/30\n",
      "2s - loss: 0.0648 - acc: 0.9809 - val_loss: 0.0387 - val_acc: 0.9873\n",
      "Epoch 20/30\n",
      "2s - loss: 0.0596 - acc: 0.9829 - val_loss: 0.0520 - val_acc: 0.9822\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00020: reducing learning rate to 0.0005000000237487257.\n",
      "3s - loss: 0.0528 - acc: 0.9854 - val_loss: 0.0365 - val_acc: 0.9886\n",
      "Epoch 22/30\n",
      "2s - loss: 0.0455 - acc: 0.9866 - val_loss: 0.0333 - val_acc: 0.9898\n",
      "Epoch 23/30\n",
      "2s - loss: 0.0395 - acc: 0.9874 - val_loss: 0.0335 - val_acc: 0.9914\n",
      "Epoch 24/30\n",
      "2s - loss: 0.0405 - acc: 0.9878 - val_loss: 0.0247 - val_acc: 0.9927\n",
      "Epoch 25/30\n",
      "2s - loss: 0.0304 - acc: 0.9907 - val_loss: 0.0341 - val_acc: 0.9905\n",
      "Epoch 26/30\n",
      "2s - loss: 0.0393 - acc: 0.9881 - val_loss: 0.0410 - val_acc: 0.9886\n",
      "Epoch 27/30\n",
      "2s - loss: 0.0349 - acc: 0.9887 - val_loss: 0.0358 - val_acc: 0.9902\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 00027: reducing learning rate to 0.0002500000118743628.\n",
      "2s - loss: 0.0339 - acc: 0.9884 - val_loss: 0.0311 - val_acc: 0.9898\n",
      "Epoch 29/30\n",
      "2s - loss: 0.0323 - acc: 0.9895 - val_loss: 0.0331 - val_acc: 0.9902\n",
      "Epoch 30/30\n",
      "2s - loss: 0.0301 - acc: 0.9897 - val_loss: 0.0336 - val_acc: 0.9905\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(data_gen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                              epochs=epochs, validation_data=(X_test, y_test),\n",
    "                              verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                              callbacks=[lr_reduction, time_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы\n",
    "(3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните наилучшую модель, полученную на xgboost, с наилучшей моделью, обученной с помощью keras.\n",
    "\n",
    "Как различаются точность, logloss, время обучения? Какую модель легче и удобнее настраивать? Какая модель вам показалась наиболее гибкой?\n",
    "\n",
    "Какие плюсы и минусы вы бы отметили у двух данных моделей машинного обучения?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рассуждения \n",
    "XGBoost - очень хороший метод машинного обучения, но он не конкурент сверточным нейросетям в задачах на таких сложных объектах, как изображения, тексты, звуковые сигналы т.к. он не умеет извлекать иерархические признаки. \n",
    "Конечно, когда разговор идет о других задачах, то лучше использовать градиентный бустинг, потому что его легче настраивать, он быстрее работает, (как правило) требует меньше обучающих примеров, чем нейросети. \n",
    "\n",
    "Самая хорошая модель XGBoost сравнима по качеству с самым хорошим MLP. Сравнивать времена обучения не буду, т.к. XGBoost у меня собран под CPU, а tensorflow (вычислительный бэкенд Keras) - под GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Обучите модели, взяв всю выборку (42000 объектов). Конечно, из-за перехода к более объемной выборке правильно будет подбирать параметры заново, но можете оставить текущие параметры, увеличив число деревьев в XGBoost или число эпох в нейронной сети, слегка снизив `learning_rate`). [Скачайте с kaggle](https://www.kaggle.com/c/digit-recognizer/data) набор тестовых данных, сделайте на них предсказание для каждой модели и отправьте оба результата (submit predictions) в Kaggle.\n",
    "\n",
    "Какой результат у вас получился? Сильно ли он отличается от того, что вы видели на эксперименте?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "9s - loss: 0.0434 - acc: 0.9873 - val_loss: 0.0170 - val_acc: 0.9946\n",
      "Epoch 2/30\n",
      "8s - loss: 0.0446 - acc: 0.9872 - val_loss: 0.0161 - val_acc: 0.9939\n",
      "Epoch 3/30\n",
      "8s - loss: 0.0431 - acc: 0.9875 - val_loss: 0.0152 - val_acc: 0.9948\n",
      "Epoch 4/30\n",
      "9s - loss: 0.0426 - acc: 0.9875 - val_loss: 0.0172 - val_acc: 0.9937\n",
      "Epoch 5/30\n",
      "9s - loss: 0.0372 - acc: 0.9892 - val_loss: 0.0153 - val_acc: 0.9949\n",
      "Epoch 6/30\n",
      "9s - loss: 0.0408 - acc: 0.9880 - val_loss: 0.0140 - val_acc: 0.9952\n",
      "Epoch 7/30\n",
      "8s - loss: 0.0402 - acc: 0.9878 - val_loss: 0.0173 - val_acc: 0.9937\n",
      "Epoch 8/30\n",
      "8s - loss: 0.0390 - acc: 0.9887 - val_loss: 0.0156 - val_acc: 0.9952\n",
      "Epoch 9/30\n",
      "8s - loss: 0.0353 - acc: 0.9892 - val_loss: 0.0170 - val_acc: 0.9945\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00009: reducing learning rate to 0.0001250000059371814.\n",
      "8s - loss: 0.0354 - acc: 0.9894 - val_loss: 0.0156 - val_acc: 0.9950\n",
      "Epoch 11/30\n",
      "9s - loss: 0.0315 - acc: 0.9902 - val_loss: 0.0153 - val_acc: 0.9949\n",
      "Epoch 12/30\n",
      "9s - loss: 0.0305 - acc: 0.9908 - val_loss: 0.0148 - val_acc: 0.9948\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00012: reducing learning rate to 6.25000029685907e-05.\n",
      "9s - loss: 0.0336 - acc: 0.9900 - val_loss: 0.0146 - val_acc: 0.9950\n",
      "Epoch 14/30\n",
      "8s - loss: 0.0317 - acc: 0.9908 - val_loss: 0.0143 - val_acc: 0.9950\n",
      "Epoch 15/30\n",
      "8s - loss: 0.0286 - acc: 0.9917 - val_loss: 0.0132 - val_acc: 0.9961\n",
      "Epoch 16/30\n",
      "8s - loss: 0.0301 - acc: 0.9911 - val_loss: 0.0140 - val_acc: 0.9952\n",
      "Epoch 17/30\n",
      "8s - loss: 0.0292 - acc: 0.9915 - val_loss: 0.0144 - val_acc: 0.9951\n",
      "Epoch 18/30\n",
      "8s - loss: 0.0311 - acc: 0.9909 - val_loss: 0.0130 - val_acc: 0.9956\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00018: reducing learning rate to 3.125000148429535e-05.\n",
      "8s - loss: 0.0300 - acc: 0.9909 - val_loss: 0.0145 - val_acc: 0.9951\n",
      "Epoch 20/30\n",
      "8s - loss: 0.0266 - acc: 0.9918 - val_loss: 0.0134 - val_acc: 0.9956\n",
      "Epoch 21/30\n",
      "8s - loss: 0.0302 - acc: 0.9909 - val_loss: 0.0137 - val_acc: 0.9951\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00021: reducing learning rate to 1.5625000742147677e-05.\n",
      "8s - loss: 0.0278 - acc: 0.9916 - val_loss: 0.0136 - val_acc: 0.9952\n",
      "Epoch 23/30\n",
      "8s - loss: 0.0280 - acc: 0.9911 - val_loss: 0.0132 - val_acc: 0.9954\n",
      "Epoch 24/30\n",
      "8s - loss: 0.0273 - acc: 0.9921 - val_loss: 0.0132 - val_acc: 0.9955\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 00024: reducing learning rate to 1e-05.\n",
      "8s - loss: 0.0256 - acc: 0.9923 - val_loss: 0.0135 - val_acc: 0.9955\n",
      "Epoch 26/30\n",
      "8s - loss: 0.0265 - acc: 0.9927 - val_loss: 0.0135 - val_acc: 0.9954\n",
      "Epoch 27/30\n",
      "8s - loss: 0.0282 - acc: 0.9917 - val_loss: 0.0133 - val_acc: 0.9952\n",
      "Epoch 28/30\n",
      "8s - loss: 0.0287 - acc: 0.9905 - val_loss: 0.0135 - val_acc: 0.9951\n",
      "Epoch 29/30\n",
      "8s - loss: 0.0281 - acc: 0.9914 - val_loss: 0.0133 - val_acc: 0.9951\n",
      "Epoch 30/30\n",
      "8s - loss: 0.0282 - acc: 0.9922 - val_loss: 0.0131 - val_acc: 0.9951\n"
     ]
    }
   ],
   "source": [
    "digits, labels = load_data(1)\n",
    "\n",
    "digits = digits / 255.0 \n",
    "test = pd.read_csv('test.csv') / 255.0\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits, labels, test_size=0.2)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "X_train = X_train.reshape(-1,28,28,1)\n",
    "X_test = X_test.reshape(-1,28,28,1)\n",
    "\n",
    "history = model.fit_generator(data_gen.flow(X_train, y_train, batch_size=batch_size),\n",
    "                              epochs=epochs, validation_data=(X_test, y_test),\n",
    "                              verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                              callbacks=[lr_reduction, time_callback])\n",
    "\n",
    "test = np.array(test).reshape(-1,28,28,1)\n",
    "\n",
    "cnn_preds = pd.Series(np.argmax(model.predict(test), axis=1), name='Label')\n",
    "submission = pd.concat([pd.Series(range(1,28001), name='ImageId'), cnn_preds], axis=1)\n",
    "submission.to_csv('hw_cnn_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.66667, gamma=0, learning_rate=0.2,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=340, n_jobs=4, nthread=None,\n",
       "       objective='multi:softprob', random_state=12, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost = XGBClassifier(n_estimators=340, n_jobs=4, random_state=12, learning_rate=0.2, \n",
    "                        max_depth=3, gamma=0, colsample_bytree=0.66667, min_child_weight=1)\n",
    "xgboost.fit(digits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv') / 255.0\n",
    "test = test.values\n",
    "y_xgb = xgboost.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_preds = pd.Series(y_xgb, name='Label')\n",
    "submission = pd.concat([pd.Series(range(1,28001), name='ImageId'), xgb_preds], axis=1)\n",
    "submission.to_csv('hw_xgb_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle \n",
    "Сверточная сеть получила 0.99614 на паблик лидерборде. XGBoost получил 0.97028. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
